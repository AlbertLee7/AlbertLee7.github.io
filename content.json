[{"title":"《统计学习方法1——统计学习方法概论》","date":"2018-07-14T14:03:11.000Z","path":"2018/07/14/《统计学习方法1——统计学习方法概论》/","text":"1. 机器学习规划做大数据已经有2年多的时间，Spark、HBase、ElasticSearch等大数据组件也在多个项目中进行了应用实践，总想着除了在广度上多接触各类大数据开源组件之外，还应该在深度上多做些研究，因为从早期 MR引擎到Spark引擎再到FLink引擎等，各种工具层次不穷，我不想只是做一个工程开发人员，我想做数据分析方面的专家，沿着大数据应用—&gt;机器学习—&gt;深度学习的路线发展。因此后面打算利用3个月的时间，进行机器学习的一个入门学习。主要的学习思路是：《统计学习方法》—&gt;《机器学习》(西瓜书) —&gt; 《机器学习实践》与TensorFlow的使用，我会在博客上更新所学到的知识点以及学习心得，也算是一个自我鞭策过程。 2. 统计学习方法概论 统计学习是关于计算机基于数据构建概率统计模型，并运用模型对数据进行预测与分析的学科。 统计学习由监督学习、非监督学习、半监督学习和强化学习等组成。 统计学习方法包括模型的假设空间、模型选择的准则以及模型学习的算法，称其为统计学习方法的三要素，简称为模型、策略和算法。 输入空间、输出空间：输入和输出所有可能取值的集合，可以是有限元素的集合，也可以是整个欧式空间。 特征空间：特征向量用来表征每个输入的具体实例，所有特征向量存在的空间成为特征空间。 假设空间：模型属于由输入空间到输出空间的映射的集合，这个集合就是假设空间。 回归问题：输入变量与输出变量均为连续变量的预测问题。 分类问题：输出变量为有限个离散变量的预测问题。 标注问题：输入变量与输出变量均为变量序列的预测问题。 统计学习的目标在于从假设空间中选取最优的模型。 损失函数/代价函数：表征预测值f(X)与真实值Y的误差。损失函数是f(X)与Y的非负实值函数，记作L(Y, f(x))，常用的有0-1损失函数、平方损失函数、绝对损失函数和对数损失函数。 经验风险：学习的目标就是选择期望风险最小的模型，期望风险与联合分布有关，不能直径计算，实际中我们往往使用经验风险来代替期望风险。经验风险最小化(ERM)认为，经验风险最小的模型是最优的模型$$R_{emp}(f)=\\frac{1}{N}\\sum_{i=1}^NL(y_i,f(x_i))—\\text{【经验风险/经验损失】}$$ 极大似然估计就是经验风险最小化的一个例子。当模型是条件概率分布，损失函数是对数损失函数时，经验风险最小化就等价于极大似然函数估计。 结构风险：当样本容量很小使，经验风险最小化策略选取的最优模型，由于过拟合问题，测试误差往往较大。结构化风险最小化是为了防止过拟合而提出的策略。结构风险最小化等价于正则化。 $$R_{srm}(f)=\\frac{1}{N}\\sum_{i=1}^NL(y_i,f(x_i)) +\\lambda J(f) —\\text{【结构风险】}$$ ​ 其中$J(f)$为模型复杂度，模型$f$越复杂，复杂度$J(f)$就越大，反之亦然。复杂度表示了对模型的惩罚。$\\lambda&gt;=0$是系数，用以权衡经验风险和模型复杂度。结构化风险小的模型往往对训练数据和未知的测试数据都有较好的预测。 贝叶斯估计中最大后验概率估计就是结构化风险最小化的例子。当模型是条件概率分布，损失函数是对数损失函数，模型复杂度由模型的先验概率表示时，结构风险最小化就等价于最大后验概估计。 监督学习问题就变成了经验风险或结构风险函数的最优化问题。这时经验/结构风险函数就是最优化的目标函数。 过拟合：如果一味追求提高对训练数据的预测能力，所选模型的复杂度则往往会比真模型更高，这种现象称为过拟合。 正则化是模型选择的典型方法，正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化值越大。 在所有可能选择的模型中，能够很好地解释已知数据并且十分简单才是最好的模型。 交叉验证是另一种常用的模型选择方法，其基本想法是重复地使用数据，把给定的数据进行切分，将切分的数据集组合为训练集和测试集，在此基础上反复进行训练、测试以及模型选择。 生成模型由数据学习联合概率分布P(X,Y)，然后求出条件概率分布P(Y|X)，作为预测的模型，它表示了给定输入X产生输出Y的生成关系。 判别模型由数据直接学习决策函数f(X)或条件概率分布P(Y|X)作为预测的模型。 回归问题的学习等价于函数拟合：选择一条函数曲线使其很好地拟合已知数据且很好地预测未知数据。 3. 章后习题解答3.1 习题1解答统计学习的三要素为模型、策略和算法。 伯努利模型，又名两点分布或0-1分布，是一个离散型概率分布，记其为1的概率为$\\theta$，则为0的概率为$1-\\theta$， 定义Y为伯努利的实验结果，则P(Y=1)=$\\theta$，P(Y=0)=$1-\\theta$，D为样本集合。伯努利模型的似然函数则为：$$P(D|\\theta)=P(y_1,y_2,…y_n|\\theta)$$由于可以将每个样本看做独立同分布，样本集D中n个样本有k个为1，则似然函数又可转化为：$$P(D|\\theta)=P(y_1,y_2,…y_n|\\theta)=\\prod_{i=1}^nP(y_i|\\theta)=\\theta^k(1-\\theta)^{n-k}$$其中，n为样本总数，k为样本中Y=1的个数。 根据最大极大似然估计，假设空间中最优的模型为使似然函数$P(D|\\theta)$最大的模型，也就转换为了求最大值的问题。对$P(D|\\theta)$求导，并使导数为0，可得：$\\theta=\\frac{k}{n}$，由上面可知$\\theta$伯努利模型中为1的概率，因此根据极大似然估计，伯努利模型中结果为1的概率$\\frac{k}{n}$ 根据贝叶斯公式：$$P(H|D)=\\frac{P(D|H) P(H)}{P(D)}$$其中，P(H)为先验概率，P(D|H)为似然概率，P(H|D)为后验概率。 我们可以得出伯努利模型下的贝叶斯公式：$$P(\\theta|D)=P(\\theta|y_1,y_2,…y_n)=\\frac{P(y_1,y_2,…y_n|\\theta)P(\\theta)}{P(y_1,y_2,…y_n)}=\\frac{\\prod_{i=1}^nP(y_i|\\theta)P(\\theta)}{P(y_1,y_2,…y_n)}$$其中$P(\\theta)$为先验概率，我们假设$\\theta$服从$\\beta$分布，则$P(\\theta)=\\frac{\\theta^{a-1} (1-\\theta^{b-1}}{常数}$ 根据贝叶斯选择假设空间最优模型，需要使后验概率$P(\\theta|y_1,y_2,…y_n)$最大，最大后验概率就是在已知观察结果的前提下，选取使$\\theta$出现概率最大的模型。由此可知需要使$\\prod_{i=1}^nP(y_i|\\theta)P(\\theta)$最大：$$\\prod_{i=1}^nP(y_i|\\theta)P(\\theta)=\\theta^k(1-\\theta)^{n-k}P(\\theta)=\\theta^k(1-\\theta)^{n-k}\\theta^{a-1} (1-\\theta)^{b-1}$$对上式求导可得：$\\theta=\\frac{k+a-1)}{n+a+b-2}$，因此根据贝叶斯估计，伯努利模型中结果为1的概率为$\\frac{k+a-1)}{n+a+b-2}$ 3.2 习题2解答当模型是条件概率分布，损失函数为对数损失函数时，损失函数为：$$L(Y,P(Y|X))=-\\log P(Y|X)$$经验风险公式就变成了：$$R_{emp}(f)=-\\frac{1}{N}\\log\\prod_{i=1}^NP(y_i|x_i)$$这时经验风险最小化就等价于$\\prod_{i=1}^NP(y_i|x_i)$最大值，也就是让所选择的模型，保证测试集中各样本的条件概率$P(y_i|x_i)$之积最大，也就是保证各样本出现的概率最大，这正符合极大似然估计的概念。因此，也就证明了模型是条件概率分布，当损失函数是对数损失函数时，经验风险最小化等价于极大似然估计。","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"}]},{"title":"Spark读写HBase性能问题及解决方案","date":"2018-07-14T12:00:00.000Z","path":"2018/07/14/Spark读写HBase性能问题及解决方案/","text":"由于3月份换了电脑，更新博客的事情就耽搁下来，今天利用周六在家的空闲时间，在Mac上安装了hexo，总算能够正常更新博客了。 最近做了几个数据量较大的项目，在Spark读写HBase时遇到了一系列的性能问题，造成Spark程序不能正常运行，甚至出现Spark任务运行十几个小时最终报异常的情况，最终经过对HBase表的调整和Spark程序的优化，实现了性能的显著提升。下面主要讲解一下整个过程中遇到的一些问题，以及最终的解决方法。 1. HBase没有预分区导致RegionServer卡死的问题 问题描述：之前考虑到HBase表新建完默认只有一个Region，当数据量达到阈值时会自动进行Region Split操作。所以系统设计之初并没有在新建HBase表时进行预分区操作。结果当大量数据一次导入HBase时，由于Region Split并不能及时触发，造成写入压力集中在一个Region上，导致RegionServer单点负载过高，阻塞甚至卡死的问题，Spark日志会打印connection reset异常(即由于RegionServer卡死造成connection reset)，Spark任务会一直阻塞。 解决方案：针对这个问题，我们在建立HBase时通过SPLITS_FILE指定分区文件，在分区文件中划定每个region的范围。我们首先在HBase的Rowkey加入了Hash前缀，并指定了00～ff共256个Region。具体的预分区方法，可以参考博客：HBase建表时预分区的方法 2. 数据倾斜导致的HBase读写性能较差的问题问题描述： 系统设计之初对数据倾斜考虑不到位，因此没有对Rowkey加入Hash前缀，导致HBase Region读写请求严重不均衡，造成Spark读写HBase性能较慢。 解决方案： 写入HBase时对RowKey添加Hash前缀，结合HBase表的手动预分区，可以使数据均匀分布于HBase集群的各Region中，避免数据倾斜造成HBase单点负载过高，出现单点瓶颈。RowKey添加Hash前缀可以参考org.apache.hadoop.hbase.util.MD5Hash类。 3. Spark BulkLoad方式向HBase导入数据HFile频繁拆分的问题问题描述：为了提升Spark写入HBase的性能，系统设计采用了BulkLoad的方法，绕过绕过HBase的WAL机制，直接将生成的HFile文件导入HBase集群，但观察Spark任务发现两点问题： BulkLoad方式下会将HDFS上生成的HFile文件加载到HBase集群中，若某个HFile中的数据属于不同Region，会在HDFS中将该HFile拆分为多个HFile文件，可以在spark日志中发现大量的HDFS文件路径频繁split的打印信息，对写入HBase性能影响很大。 HBase要求每个HFile内的RowKey为升序排列，Spark采用BulkLoad方式在生成HFile时需要对数据进行排序，否则会导致写入HBase失败。在排序过程中，系统一开始使用SortByKey全局排序算子进行排序，这个过程性能较慢； 解决方案：Spark在生成HFile之前，使用Spark的PartitionBy算子进行自定义分区，保证每个HFile文件中的数据属于同一个Region，可以避免上面HDFS 频繁拆分HFile现象的发生。 针对SortByKey算子运行较慢的问题，由于只需要保证每个HFile内部有序即可，我们使用MapPartition算子进行局部排序，替换原有的SortByKey全局排序类算子，提高了排序性能。 4. 总结​通过这次的调优过程，最大的体会就是，上述的大部分问题在系统设计之初本就可以避免，但当初过于乐观，想着HBase会自动进行Region Split，系统中的数据也不会过于倾斜，这些估计或预想都是毫无根据，纯粹是一厢情愿，也就造成了系统正式上线后遇到了诸多性能问题，这次是非常深刻的教训，今后在系统设计时，要本着最严格的要求进行开发！","tags":[{"name":"HBase","slug":"HBase","permalink":"http://yoursite.com/tags/HBase/"}]},{"title":"2017年度工作总结","date":"2018-02-13T05:29:08.000Z","path":"2018/02/13/2017年度总结/","text":"今天是农历腊月二十八，时间快的让人猝不及防，又是一年春节，前天刚刚结束在北京为期50天的封闭开发回到沈阳公司，趁着这个闲暇时间简单总结下自己在即将过去的2017年中的工作与生活，并展望下2018年，开启新的篇章。 1. 2017年工作总结依然清晰记得去年春节的时候，公司让每位员工写下各自2017年KPI，我当时还是一个只会写Storm实时分析的初级开发人员，充满了对周围Spark高级工程师的崇拜，所以我在KPI写的是：1.深入了解Storm内部机制，做到不仅仅会使用Storm，还要做到知其所以然；2. 学习Spark，能够自己开发Spark离线分析程序和实时分析程序。现在回首过去的2017年感慨颇多，虽然我已经离开了当初的公司，但在2017年我一直在按照当初的KPI努力。 2017年3月，我阅读了Storm的部分源码，了解了storm应用程序的启动流程，了解了Nimbus/Supervisor与Zookeeper之前通过心跳包完成数据/任务同步与状态监控，了解了Supervisor与Worker通过节点本地磁盘实现数据交换/任务分配与状态监控。 2017年4月，我通过Spark官方文档学习了Spark离线开发并在君哥的指导下慢慢独自开始负责一部分Spark离线报表的产出工作，但当时也只是停留在使用阶段，不了解Spark的一些细节以及具体配置。 2017年5~6月，我在黎叔的指导下开始负责反作弊平台的一部分工作，主要是使用Zeppelin运行Spark Sql进行反作弊平台的前期数据挖掘工作，期间学会了Zeppelin的使用与安装。 2017年8~10月，我更换了工作开始在新的公司继续深入了解Spark/HBase/Hive/ElasticSearch等大数据组件的使用，并与另一个同事搭建了一个我们自己的大数据集群，在5台虚拟机上安装了Hadoop/Spark/Hive/HBase等大数据组件，并在期间对公司员工进行了一个小型的Spark技术分享，这个过程使我能够静下心来好好沉淀一下，对Spark的一些关键概念与主要结构有了更深入的理解，并对其他大数据组件有了更多的了解，大大扩展了我在大数据领域的知识面。更为重要的是，通过自己动手搭建一套大数据开发集群，使我对各大数据组件之前的联系有了一个较为深入的了解。 2017年11~12月，正好有个SparkStreaming的项目，我利用这个难得的机会，把我8~10月份在Spark方面的积累通通在这个项目中进行了实践，包括SparkStreming实时分析开发，CheckoutPoint机制，Spark应用程序在Yarn的重启配置等，以及Spark读取Kafka等内容，虽然项目不大，但给了我一个很好的实践机会。 2018年1月~至今，北京总公司正好有个比较大的大数据开发项目，因为之前看了很多理论知识，一直想找个实践的机会的大展拳脚，于是便排除困难，去北京出差50年进行封闭开发去了(不得不说北京996的工作强度的确不小，让我现在一点都不羡慕北京IT的工作了)。在北京的封闭开发中，我主要负责标签体系中Spark离线标签产出功能模块，在这个功能模块中，我通过Spark操作Hive/HBase/ElasticSearch等大数据组件，我是真正的将前期积累的理论知识全部无一例外的实践了一遍，除了使用Scala还学会了使用Java进行Spark开发。虽然工作较累，但与收获相比还是值得的。现在，虽然我还不是大数据方面的专家大神，但我完成了去年制定的2017kPI，对大数据各组件均有了一定了解，无论是大数据离线开发还是实时开发，都能够独自完成，算是及格了吧。仔细算来，自己工作已经一年半，越来越体会到，在学校老师一直教导我们待人接物要谦逊温和，然而现实社会就是如此残酷，处处都是谦逊温和之人吃亏，现在的人不会认为谦逊是一种美德，反而是一种老实可欺的表现。也许是还没到达那个高度吧，不仅没有自傲的资格，连谦逊的资格都没有，需要处处充满狼性，处处与人斗，你退一步别人都会认为你是可欺的。呵呵，就是这么滑稽工作技能需要提升，生活方面性格方面也必须要努力改变。。还是太年轻，努力吧自己，奋斗吧少年，还差的远呢。(PS:我上家公司是一个非常好的公司，我当时所在的团队也是一个非常非常不错的团队，感谢杰哥，黎叔，君哥对我照顾，带我走进大数据领域，虽然只待了区区一年，但作为我的第一份工作，收获巨大，难忘的美好回忆。)2. 2018年工作展望2018年有好多好多想要做的事情，有好多好多需要学习的东西，想想就兴奋，哈哈，程序猿就是这么苦逼，学海无涯啊，苦中作乐吧。2018年主要工作(学习)核心： 需要学学Python，因为机器学习和深度学习都需要Python语言； Spark源代码阅读，好奇Spark源码，决定一探究竟； 需要学学机器学习，学会使用TensorFlow进行机器学习开发； 需要学学Spring与Spring Boot开发，沈阳纯粹大数据开发的岗位不像北京等一线城市那么多，所以需要进一步扩大自己的知识面，而Spring与Spring Boot的确是一个不错的东西，值得学习一下。其他方面，我以后不能总像个孩子那样，充满童真了，也不能那么佛系了，毕竟做了父亲，有了更多的压力与责任，更需要一个成熟有担当的自己！加油吧，趁年轻，多尝试自己畏惧的东西，别等30岁了还没有成为自己想成为的那个人，看不起自己。","tags":[{"name":"随笔","slug":"随笔","permalink":"http://yoursite.com/tags/随笔/"}]},{"title":"《深入理解JVM虚拟机》阅读总结","date":"2017-12-11T02:37:11.000Z","path":"2017/12/11/《深入理解JVM虚拟机》阅读总结/","text":"1. 写在前面实际项目中，编写Java程序时一直都停留在应用阶段，至于代码编译后是什么样子，类文件如何加载，如何执行，JVM内存分配与垃圾回收等内容，一直都是一知半解，总想着找机会读一下《深入理解Java虚拟机》这本书。近段时间，总算是有了些许空余时间，怀着略显饥渴的心情，在两周时间里断断续续的读完了这本书。然而才过了这么几天，对书中讲解的知识点又忘记的差不多了(吐血ing…)。也许读书就是这样吧，读第一遍只能记住书的大概，只有在实际项目中遇到问题回头翻阅这本书时去读第二遍以至第三遍，才能完完全吸收书中的知识点。今天，借此机会，简单总结一下书中知识点，方便以后回顾，也算是为第一遍阅读《深入理解Java虚拟机》画下一个句号。 2. JVM内存分布上图为JVM运行时内存分布，主要虚拟机栈、本地方法栈、程序计数器、JVM堆和方法区。其中堆和方法区是Thread共享的，虚拟机栈、本地方法栈和程序计数器是Thread独享的。 2.1 程序计数器程序计数器：每个Thread都有程序计数器，用来保存代码的执行位置。 2.2 虚拟机栈虚拟机栈中分布有栈帧，每个方法对应一个栈帧，方法的运行伴随着栈帧的入栈和出栈操作。栈帧中保存有局部变量表、操作数栈、返回地址、动态连接等信息。局部变量表在代码编译成类文件后会指定需要多个slot，局部变量表的大小就确定了，方法的参数、外部引用、this引用等都保存在局部变量表中。代码运行时需要进行入栈和出栈操作，这些都需要使用操作数栈来实现。 2.3 本地方法栈JVM中Thread非java代码的运行不在虚拟机栈中运行，而是在本地方法栈中运行。 2.4 堆堆是JVM中进行垃圾收集的主站场，JVM中new对象时都需要在堆中申请相应的内存，垃圾回收机制定期对堆中内存进行收集，为了能够更好的管理堆中内存，将内存分为了新生代、老生代，并发展了各种垃圾收集器，以满足不同需要。这其中涉及各种回收算法和垃圾收集器等内容，我们将在下节详细讲解。 2.5 方法区JVM启动后，当类被第一次使用时JVM会将该类的各种信息以及编译完成的代码放入方法区中，即类加载。在堆中新建类对象时，需要在方法区中获取该类的各种信息。方法区的内存常常被称为永久代，但实际上垃圾收集机制并不是不收集方法区的内存，仅仅是频率较低，且收集较为困难。 3. 垃圾收集机制垃圾回收机制实际上主要解决了三个问题： 哪些属于需要回收的垃圾？ 如何回收垃圾？ 何时回收垃圾？3.1 可达性分析算法判断一个对象是否需要回收，之前提出过引用计数算法，即计数每个对象的被引用次数，但该算法存在缺陷，但A和B对象相互引用时，A和B则永远不会被回收，因此这种引用计数算法是不可行的。可达性分析算法是目前使用的垃圾标定算法，该算法通过一系列称为“GC Roots”的对象作为起始点，从这些节点向下搜索，搜索所走过的路径称为引用链“Reference Chain”，当一个对象到GC Roots没有任何引用链相连时，即从GC Roots到这个对象不可达，则证明该对象是不可用的。在Java语言中，可作为GC Roots的对象包括下面几种： 虚拟机栈(栈帧中的本地变量表)中引用的对象； 方法区中类静态属性引用的对象； 方法区中常量引用的对象； 本地方法栈中JNI(即Native方法)PS:这里的引用，与C语言中引用非常类似。可达性分析算法要求在对堆中对象进行可达性分析时，需要暂停整个进程即Stop The World，这往往会导致运行效率低下。后期的垃圾收集器针对该问题提出了多种改进，产生了多种垃圾收集器。3.2 垃圾收集算法 标记-清理算法：该算法对标记为可收集的内存进行回收，优点是速度快，缺点是容易产生大量内存碎片，会出现无较大连续内存可用的情况。 复制算法：该算法首先将堆内存分为两部分如A和B，A中有数据，B为空白内存，垃圾收集时将A中存活对象复制到B中并清空A，下一轮收集则将B中存活对象复制到A中并清空B。该算法优点为效率较高且会有连续可用内存，缺点是需要将内存一分为二，造成内存资源浪费。后期将该算法做了改进，用于堆中新生代内存的收集算法。主要改进为，结合新生代内存每次存活对象较少的特点，将新生代内存划分为Eden、Survivor1和Survivor2三部分Eden较大，两个Survivor较小，每次收集将Eden与其中一个Survivor(如Survivor1)中存活对象放入空白的另一个Survivor(如Survivor2)，并清空Eden与Survivor1，新建对象时只从Eden中分配内存。这样的话即可对复制算法扬长避短，收集效率较高。同时，当新生代存活对象较大，Survivor存放不下时，会触发老年代内存担保机制，即将新生代存活的对象放入老年代中。 标记-整理算法：该算法首先使用标记-清理算法，然后将存活对象移向内存一侧，并记录内存的偏移，分配新内存时从偏移处开始分配新的内存。该算法适合存活对象较多的情况，主要用于对老年代内存的收集。需要注意的是老年代没有担保机制。 分代收集算法：该算法主要是根据数据的不同特点，将堆内存分为新生代和老年代，新生代内存的特点是频繁新建与回收，每次存活的对象较少，适合改进后的复制算法，老年代内存的特点是对象的存活时间较长，不会频繁新建与回收，因此适合标记-整理算法或标记清理算法。同时，当新生代内存存活的对象较多，Survivor放不下时，会触发内存担保机制，将新生代内存存活对象放入老年代内存。老年代内存中的数据是从新生代转换而来。若老年年内存依然不够，则会触发对老年代内存的垃圾收集，若依然不够，则会导致OutofMemory异常。3.3 垃圾收集时间点 新生代GC（Minor GC）:指发生在新生代的垃圾收集动作，因为Java对象大多都具备朝生夕灭的特性，所以Minor GC非常频繁，一般回收速度也比较快。 老年代GC（Full GC)：指发生在老年代的GC。 Minor GC触发条件：当Eden区满时，触发Minor GC。 Full GC触发条件：（1）调用System.gc时，系统建议执行Full GC，但是不必然执行（2）老年代空间不足（3）方法区空间不足（4）通过Minor GC后进入老年代的平均大小大于老年代的可用内存（5）由Eden区、From Space区向To Space区复制时，对象大小大于To Space可用内存，则把该对象转存到老年代，且老年代的可用内存小于该对象大小3.4 垃圾收集器介绍 Serial收集器：单线程垃圾收集器，进行垃圾收集时，必须暂停其他所有的工作线程。 ParNew收集器：Serial收集器的多线程版本 Parallel Scavenge收集器与ParNew相似，但可以达到一个可控制的吞吐量(CPU运行用户代码时间/CPU总消耗时间) Serial Old收集器：Serial收集器的老年代版本，使用“标记-整理算法” Parallel Old收集器：Parallel Scavenge收集器的老年代版本，使用多线程和“标记-整理算法” CMS收集器：一种以获取最短回收停顿时间为目标的收集器，使用“标记——清除算法”。整个收集过程分为4个步骤：(1)初始标记；(2)并发标记；(3)重新标记；(4)并发清除 G1收集器是当今收集器技术发展的最前沿成果之一，特点有：并行与并发、分代收集、空间整合、可预测的停顿等。使用G1收集器时，Java堆的内存布局与其他收集器有很大差别，它将整个Java堆划分为多个大小相等的独立区域(Region)，虽然还保留有新生代和老年的的概念，但新生代和老年代不再是物理隔离的了，它们都是一部分Region(不需要连续)的集合。4. 类文件结构代码编译的结果从本地机器码转变为字节码，是存储格式发展的一小步，却是编程语言发展的一大步。Class文件格式所具备的平台中立(不依赖于特定硬件及操作系统)、紧凑、稳定和可扩展的特点，是Java技术体系实现平台无关、语言无关两项特性的重要支柱。Java虚拟机不和包括Java在内的任何语言绑定，它只与“Class文件”这种特定的二进制文件格式所关联，Class文件包含了Java虚拟机指令集和符号表以及若个其他辅组信息。Class文件格式采用一种类似C语言结构体的伪结构来存储数据，这种伪结构中只有两种数据类型：无符号数和表。无符号数属于基本数据类型，以u1，u2，u4，u8来分别代表1个字节、2个字节、4个字节和8个字节的无符号数，无符号数可以用来描述数字、索引引用、数量值或者按照UTF-8编码构成字符串值。表是由多个无符号数或者其他表作为数据项构成的复合数据类型。表用于描述有层次关系的复合结构的数据，整个Class文件本质上就是一张表。Class中以_info结尾代表一张表。4.1 魔数每个Class文件的头4个字节称为魔数（Magic Number），它唯一作用就是用来确定文件是否能被虚拟机接受，值为“CAFEBABE”4.2 版本号接下来的4个字节存储着Class文件的版本号，第五第六个字节为次版本号（Minor Version），第七第八为主版本号（Major Version）。版本号主要用于版本控制，高版本的JDK能向下兼容以前版本的Class文件，但不能运行以后版本的Class文件。4.3 常量池常量池是一个表类型的数据项，相当于Class文件的资源仓库，与Class文件其他项目关联最多，占用Class空间最大的数据项之一，且是第一个出现的表类型数据项目。4.4 访问标志常量池之后就是由两个字节代表的访问标志（access flags）这些标志用于识别一些类或者接口层次的访问信息，包括这个Class是类还是接口；是否定义为public；是否定义为abstract类型；是否被final修饰。4.5 类索引、父类索引与接口索引集合访问标志位之后就是u2类型的类索引，父类索引和接口索引集合。Class文件由这三项数据确定这个类的继承关系。这三项数据（u2类型的索引值）各指向类型为CONSTANT_Class_info的类描述符常量。4.6 字段表集合字段表用于描述接口或者类中声明的变量。字段（field）包括类级变量以及实例级变量，但不包括在方法内部声明的局部变量。字段表中字段的各种描述信息（作用域比如public，private，是否被final，static修饰，是否可序列化等）均使用标志位表示，名称则引用常量池中的常量来描述。4.7 方法表集合在方法表中，方法的描述和字段的描述基本一致，依次包括访问标志（access_flags）、名称索引（name_index）、描述符索引（descriptor_index）、属性表集合（attributes）几项。方法中的代码经过编译器编译成字节码指令后存放在方法属性表集合中一个名为“Code”的属性里面。如果父类方法在子类中没有被重写，方法表集合中就不会出现来自父类的方法信息。4.8 属性表集合Class文件、字段表、方法表都可以携带自己的属性表集合，以用于描述某些场景专有的信息。为了能正确解析Class文件，在Java SE 7中预定义了21项属性，虚拟机在运行时会忽略他不认识的属性。5. JVM类加载机制如上图所示，类从被加载到虚拟机内存中开始，到卸载出内存为止，它的整个生命周期包括：加载、验证、准备、解析、初始化、使用和卸载七个阶段。5.1 加载 通过一个类的全限定名来获取其定义的二进制字节流。 将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构。 在Java堆中生成一个代表这个类的java.lang.Class对象，作为对方法区中这些数据的访问入口。5.2 验证验证是连接阶段的第一步，这一阶段的目的是为了确保Class文件的字节流中包含的信息符合当前虚拟机的要求，并且不会危害虚拟机自身的安全。 文件格式验证：(1) 是否以魔数0xCAFEBABE开头；(2) 主、次版本号是否在当前虚拟机处理范围之内;(3) 常量池的常量中是否有不被支持的常量类型(检查常量tag标志)；(4) 指向常量的各种索引值中是否有指向不存在的常量或不符合类型的常量；(5) CONSTARTN_Utf8_info型的常量中是否有不符合UTF8编码的数据；(6) Class文件中各个部分及文件本身是否有被删除的或附加的其他信息。 元数据验证：(1) 这个类是否有父类(除了java.lang.Object之外，所有的类都应当有父类)；(2) 这个类的父类是否继承了不允许被继承的类(被final修饰的类)；(3) 如果这个类不是抽象类，是否实现了其父类或接口之中要求实现的所有方法；(4) 类中的字段、方法是否与父类产生矛盾(例如覆盖了父类的final字段，或者出现不符合规则的方法重载，例如方法参数都一致，但返回值类型却不同等)。 字节码验证(1) 保证任意时刻操作数栈的数据类型与指令代码序列都能配合工作，例如不会出现类似这样的情况：在操作栈放置了一个int类型的数据，使用时去按long类型来加载入本地变量表中；(2) 保证跳转指令不会跳转到方法体以外的字节码指令上；(3) 保证方法体中的类型转换是有效的。 符号引用验证(1) 符号引用中通过字符串描述的全限定名是否能找到对应的类；(2) 在指定类中是否存在符合方法的字段描述符以及简单名称所描述的方法或字段；(3) 符号引用中的类、字段、方法的访问性(private、protected、public、default)是否可被当前类访问。5.3 准备准备阶段是正式为类变量分配内存并设置类变量初始值的阶段，这些变量所使用的内存都将在方法区中进行分配。需要注意的是，这时候进行内存分配的仅包括类变量(被static修饰的变量)，而不包括实例变量，实例变量将会在对象实例化时随着对象一起分配在Java堆中。另外，这里所说的初始值“通常情况”下是数据类型的零值，而不是程序赋值的初始值。5.4 解析解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程。直接引用是直接指向目标的指针、相对偏移量或是一个能间接定位到目标的句柄。这个过程中涉及对类或接口的解析、对字段的解析、对类方法的解析、对接口方法的解析。5.5 初始化类初始化阶段是类加载过程的最后一步，前面的类加载过程中，除了在加载阶段用户应用程序可以通过自定义类加载器参与之外，其余动作完全由虚拟机主导和控制。到了初始化阶段，才真正开始执行类中定义的Java程序代码(或者说是字节码)。6. JVM字节码执行上图为运行时栈帧结构，栈帧是用于支持虚拟机进行方法调用和方法执行的数据结构，它是虚拟机运行时数据区的虚拟机栈的栈元素。栈帧存储了方法的局部变量表、操作数栈、动态连接和方法返回地址和一些额外的附加信息。每个方法从调用开始至执行完成的过程，都对应着一个栈帧在虚拟机栈里面从入栈到出栈的过程。6.1 局部变量表局部变量表是一组变量值存储空间，用于存放方法参数和方法内部定义的局部变量。在Java程序编译为Class文件时，就在方法的Code属性的max_locals数据项中确定了该方法所需要分配的局部变量表的最大容量。6.2 操作数栈操作数栈也常称为操作栈，它是一个后入先出栈。同局部变量表一样，操作数的最大深度也在编译的时候写入到Code属性的max_stacks数据项中。当一个方法刚开始执行的时候，这个方法的操作数栈时空的，在方法的执行过程中，会有各种字节码指令往操作数栈中写入和提取内容，也就是出栈/入栈操作。6.3 动态连接每个栈帧都包含一个指向运行时常量池中该栈帧所属方法的引用，持有这个引用是为了支持方法调用过程中的动态连接。6.4 方法返回地址当一个方法开始执行后，只有两种方式可以退出这个方法。第一种方式是执行引擎遇到任意一个方法返回的字节码指令，这时候可能会有返回值传递给上层的方法调用这，取决遇到何种方法返回指令，这种方式称为正常完成出口。另一种退出方式是，在方法执行过程中遇到了异常，并且这个异常没有在方法体内得到处理，无论是Java虚拟机内部产生的异常，还是代码中使用了athrow字节码指令产生的异常，只要在本方法的异常表中没有搜索到匹配的异常处理器，就会导致方法退出，这种退出方法的方式称为异常完成出口，一个方法使用异常完成出口的方式退出，是不会给它的上层调用者产生任何返回值的。 参考文献GC详解及Minor GC和Full GC触发条件总结Class类文件结构分析","tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"}]},{"title":"SparkStreaming实践总结","date":"2017-12-08T06:13:22.000Z","path":"2017/12/08/SparkStreaming实践总结/","text":"1. 写在前面上个月做了一个SparkStreaming的小项目，数据量很小，功能也比较单一，从Kafka读取数据经过简单处理，将最终的数据落地到Mysql中。虽然项目小，但这是我第一个SparkStreaming项目，通过这个简单实践，让我对SparkStreaming应用和Scala编程都有了一个更深的认识。这个过程中，遇到一些问题，也引发了一些思考，记录于此，算是对这个项目的简单总结吧，保密起见，文中代码都是网上找的一些示例代码，不涉及项目实际代码。 2. SparkStreaming读取Kafka数据2.1 Kafka High Level Consumer方式从Kafka读取数据有两种方式，第一种方式是使用High Level Consumer API接口，该接口中除了有Kafka Topic的概念，还包含Consumer Group的概念，每个Topic的每个Group的Offset都保存在Zookeeper，由ZK负责管理Offset。用户通过访问Zookeeper来获得Offset，从而进行数据读取，该模式下可以实现数据的广播分发和单次分发等功能。该模式的优势在于Offset交由Zookeeper管理，用户代码实现简单，只需设置好Topic和Group（若Group不存在，Zookeeper会自动新建），与Zookeeper通讯即可，不需要关系Offset。缺点在于容易导致数据丢失或数据重复问题。同时需要注意的是，同一个Topic和Group的每个Partion中数据只能由一个Consumer消费，Kafka针对这个问题有一个Consumer Rebalance功能，可以根据Cousmer与Partion的数量关系，进行不同的数据分配机制，具体可以在网上查询相关资料。 2.2 Kafka Low Level Consumer方式从Kafka读取数据的第二种方式为使用Low Level Consumer API，虽然是Low Level，但该API赋予了用户更多的权限，用户直接管理Topic的Offset，不存在Group的概念。由于Kafka不会删除数据，所有该方式下的优势即为读取效率更高，不会出现数据丢失和重复的问题，缺点是需要用户代码完成Topic Offset的管理与备份，实现较为麻烦。 2.3 SparkStreaming 读取Kafka数据SparkStreaming读取Kafka数据也根据High Level和Low Level提供了两种读取方式。Receiver-based Approach采用High Level，只需指定Zookeeper Quorum、Group以及Map(topic-&gt;consumerNum)即可，该方式优点是实现简单方便，可以实现At Least Once，SparkStreaming会有多个Receiver作为Consumer消费Kafka Topic中数据，并将数据放入Executor内存中，SparkStreaming的Partion与Kafka的Partion无关，不存在对应关系。该方式的缺点是为了保证数据不会在SparkStreaming异常时丢失，需要启动SparkStreaming的CheckoutPoint机制，即WAL机制，这样会造成数据在Kafka和Spark出现重复保存，浪费磁盘空间，同时读取效率也会降低。1234import org.apache.spark.streaming.kafka._val kafkaStream = KafkaUtils.createStream(streamingContext, [ZK quorum], [consumer group id], [per-topic number of Kafka partitions to consume]) Direct Approach采用Low Level，该方式下SparkStreaming的Partion与Kafak的Partion一一对应，SparkStreaming读取数据效率会非常快，且SparkStreaming自主管理Kafka的Offset，可以实现Exactly-once。但为了保证SparkStreaming在异常时不会丢失内存中的Kafka Offset，需要用户自己实现Offset的容错，常见的是用户将Offset同步到Zookeeper中备份。12345import org.apache.spark.streaming.kafka._val directKafkaStream = KafkaUtils.createDirectStream[ [key class], [value class], [key decoder class], [value decoder class] ]( streamingContext, [map of Kafka parameters], [set of topics to consume]) 本项目中，由于数据量较小，同时考虑到实现的简便性，我们采用了SparkStreaming Receiver-based Approach方式，并添加了CheckoutPoint机制，用户可以通过配置文件conf.properties选择是否开启CheckoutPoint机制。 3. SparkStreaming CheckoutPoint机制SparkStreaming的CheckoutPoint机制是为了保证数据不丢失将Streaming中每批RDD的数据保存到HDFS中，当该批次RDD处理完成后再从HDFS中删除的机制，类似WAL机制。因此CheckoutPoint机制需要在创建StreamingContext时调用 StreamingContext.getOrCreate(path,creatingFunc)方法，该方法在Driver程序启动并创建StreamingContext时首先从指定的HDFS路径中判断是否存在备份数据，若存在则从备份数据中重启StreamingContext，若不存在则新建StreamingContext。因此再使用CheckoutPoint时务必设置好HDFS的文件路径，否则程序会报异常。另一方面，Yarn集群中默认Application失败重启的次数为2，所以SparkStreaming再失败从CheckoutPoint重启时，只能重启一次，为了增加重启次数，需要修改Yarn的配置mapreduce.am.max-attemptsMR ApplicationMaster最大失败尝试次数，默认为2，我们可以根据实际情况增大该参数。 4. SparkStreaming 对象序列化问题在SparkStreaming程序中我们使用到了JDBC连接池以及Logger日志打印等功能，Spark官网中建议对JDBC连接池实例进行lazy懒惰创建处理，而且实际应用中Logger的实例也需要加入@transient注解，确保Logger在序列化时不会被序列化。针对这个问题，我的猜想是若程序中，未在Drvier中使用而是在分布式算子中使用了JDBC连接池，则为了保证JDBC不会在Drvier实例化，需要在分布式算子中对JDBC连接池实例化代码加入lazy关键字，保证每个Executor中新建一个JDBC连接池实例。Logger由于我们在Driver和分布式算子中都使用到了，因此Drvier会将进程中的Driver的Logger对象进行序列化，并传输给各Executors，然而Logger是无法序列化的，程序会出错。解决方法就是在Logger字段中加入@transient注解，指明该字段在序列化时不进行序列化。这样即使在Driver中使用实例化Logger对象，在序列化时Logger对象也不会序列化进去，Executors中仍然会新建一个Logger对象。序列化问题的根本原因是：在使用到分布式算子时，Diver会将进程内存中的对象序列化并传输到各Executors中，若有些类无法序列化，则这个序列化过程中会报序列化异常，为了避免该情况的发生，则需要lazy关键字和@transient注解的配合，若类只在Executors中使用，则加入lazy保证只在Executor中实例化即可。若类在Driver和Executors中均被使用，则需要加入@transient注解和lazy关键字，保证该类实例在Executor中新建，而不使用Driver中的实例。分布式计算由于涉及到在各分布式节点中传输数据和程序，因此会出现一系列问题，在实际应用中尤其要注意这点。 5. Spark在Yarn集群的内存分配问题当我们将程序编译成jar包使用spark-submit向Yarn集群提交作业后，在Yarn ResourceManager Web UI上发现，任务所占用的内存远比我们设定的要大，比如我们设定--drvier-memory 1G --executor-memory 1G --num-executors 2正常来讲，Yarn只需要分配3G内存即可，但Yarn集群实际分配内存为6G，我一度怀疑是自己编写的代码存在Bug，但查询资料发现是Yarn资源分配机制的原因所造成的。在设定完drvier-memory和executor-memory以后，Yarn还需要为driver和每个executor分配额外内存用于运行JVM进程，因此总内存较指定的分配内存要大，同时Yarn集群设置了Container最小分配内存，这就造成了上述明明只需3G，却分配了6G内存的问题。 spark.driver.memory：默认值512m spark.executor.memory：默认值512m spark.yarn.am.memory：默认值512m spark.yarn.executor.memoryOverhead：值为executorMemory * 0.07, with minimum of 384 spark.yarn.driver.memoryOverhead：值为driverMemory * 0.07, with minimum of 384 spark.yarn.am.memoryOverhead：值为AM memory * 0.07, with minimum of 384–executor-memory/spark.executor.memory 控制 executor 的堆的大小，但是 JVM 本身也会占用一定的堆空间，比如内部的 String 或者直接 byte buffer，spark.yarn.XXX.memoryOverhead属性决定向 YARN 请求的每个 executor 或dirver或am 的额外堆内存大小，默认值为 max(384, 0.07 * spark.executor.memory) yarn.app.mapreduce.am.resource.mb：AM能够申请的最大内存，默认值为1536MB yarn.nodemanager.resource.memory-mb：nodemanager能够申请的最大内存，默认值为8192MB yarn.scheduler.minimum-allocation-mb：调度时一个container能够申请的最小资源，默认值为1024MB yarn.scheduler.maximum-allocation-mb：调度时一个container能够申请的最大资源，默认值为8192MB关于这个话题的具体信息可以参考Spark on Yarn的内存分配问题和Spark配置参数6. c3p0 JDBC连接 8小时失效问题该项目中需要在SparkStreaming程序中将数据写入Mysql，采用了c3p0 JDBC连接池的方式。一开始由于测试数据较少，一天都可能没有一条数据，所以出现了JDBC连接池中的Connection连接在第二天失效的问题，即使用前一天建立的Connection向Mysql写入数据时报出异常，提示连接失效。上网查询资料后发现这是Mysql的连接8小时自动失效导致的问题。只需对c3p0稍加配置即可。 idleConnectionTestPeriod = 3600 每60秒检查所有连接池中的空闲连接 testConnectionOnCheckout = false 因性能消耗大请只在需要的时候使用它。如果设为true那么在每个connection提交的时候都将校验其有效性。建议使用idleConnectionTestPeriod或automaticTestTable等方法来提升连接测试的性能。 testConnectionOnCheckin = true如果设为true那么在取得连接的同时将校验连接的有效性。Default: false关于c3p0详细配置可以参考c3p0详细配置7.总结通过这个项目，更加体会到了只看书学理论是远远不能掌握所有要点的，且容易忘记，只有结合项目实践，从实践中体会书中所提到的理论知识，才能真正的将知识点化为自身内在的东西。","tags":[{"name":"Spark","slug":"Spark","permalink":"http://yoursite.com/tags/Spark/"}]},{"title":"Apache Livy介绍","date":"2017-10-17T03:14:38.000Z","path":"2017/10/17/Apache Livy介绍/","text":"1. Apache Livy简介1.1 Livy与Zeppelin对比在实际应用中，Spark SQL可以快速的完成对大数据量的统计分析，但Spark SQL对交互式查询支持的并不友好，因此也就衍生出多种第三方组件，如Apache Zeppelin、Apache Livy等。Zeppelin提供了一个非常友好的 WebUI 界面以及操作指令，可用于做数据分析和可视化。其后面可以接入不同的数据处理引擎，包括 Flink，Spark，Hive 等，利用Scala语言、SQL语言等，可以方便的进行数据查询，将结果可视化，并可以实现多数据源(HDFS、HBase、MySQL)的联合分析。但Zeppelin的劣势在于只提供了WebUI的接口，并为提供REST/Java接口，因此无法将Zeppelin集成在我们的系统中，只能作为一个第三方服务存在，定制化较差。并且，Zeppelin的一个实例对应Spark中的一个Spark Application，Zeppelin运行过程中，需要始终保持该Application，即使在Zeppelin闲置时，所占用的Spark资源依然无法释放，这就造成了Spark资源的严重浪费。与之相比，Apache Livy要轻量的多，并且Livy提供了REST接口和Java接口，可以很方便的集成于我们的系统中，为Spark提供一个更上一级的接口，隐藏Spark任务提交的细节。 1.2 Apache Livy特点与架构Apache Livy中一个Session对应Spark中的一个Application，Session中的Statement对应Spark Application中的Job。当Session闲置超过一定时间时，Livy会自动将其关闭，Spark中对应的Application也就关闭。Apache Livy提供了以下基本功能： 提交Scala、Python或是R代码片段到远端的Spark集群上执行。 提交Java、Scala、Python所编写的Spark作业到远端的Spark集群上执行; 提交批处理应用在集群中运行。Livy是一个典型的REST服务架构，它一方面接受并解析用户的REST请求，转换成相应的操作;另一方面它管理着用户所启动的所有Spark集群。2. Apache Livy搭建 从Apache Livy官网下载压缩包并解压，进入Livy安装目录的conf目录下。 在conf目录下修改livy.conf文件，并添加如下设置： 1234567891011121314151617#设置livy server的addresslivy.server.host = 172.24.2.XXX#设置livy server的portlivy.server.port = 8998#设置livy sessions使用的spark masterlivy.spark.master = yarn#设置livy sessions使用的spark deploy modelivy.spark.deploy-mode = cluster#设置livy rsc jars的地址，若不设置则默认每次启动一个session，都需要将livy本地目录中的rsc jars上传至spark集群，设置为hdfs目录路径，则省钱上次过程，session启动更加迅速livy.rsc.jars = hdfs:///spark/lib/livy-api-0.4.0-incubating.jar,hdfs:///spark/lib/livy-rsc-0.4.0-incubating.jar,hdfs:///spark/lib/netty-all-4.0.29.Final.jar#设置livy repl jars的地址，若不设置则默认每次启动一个session，都需要将livy本地目录中的repl jars上传至spark集群，设置为hdfs目录路径，则省钱上次过程，session启动更加迅速。有livy-repl_2.10 and livy-repl_2.11 jars两种版本，这里我们spark使用scala2.10，因此我们使用2.10版本livy.repl.jars = hdfs:///spark/lib/commons-codec-1.9.jar,hdfs:///spark/lib/livy-core_2.10-0.4.0-incubating.jar,hdfs:///spark/lib/livy-repl_2.10-0.4.0-incubating.jar 在conf目录下的livy-env.sh中设置JAVA_HOME、HADOOP_CONF_DIR、SPARK_HOME、SPARK_CONF_DIR等配置，与java、hadoop和spark的设置相对应。 在bin目录下启动livy-server。livy-server启动后，我们可以在浏览器中输入网址：172.24.2.XXX:8998/ui 查看livy中启动的sessions以及运行结果等。3. Apache Livy REST APIApache Livy REST API 官方文档详细介绍了Livy REST API的用法。本节我们主要针对常用接口简要介绍。我们可以通过流量器插件Postman调用Livy的REST API来使用Livy，完成Session创建、查看Session状态信息、在Session创建Statement、查看Statement状态信息和运行结果以及删除Session等操作。3.1 Post SessionsLivy创建Sessions通过Post Sessions命令完成，一个Session对应Spark中的一个Application，因为我们可以指定kind即类型spark/pyspark等、指定yarn的queue以及通过jars指定spark作业外部依赖的jar包等。具体支持的设置参数参加Livy官网。Post Sessions成功后会返回所创建Session的SessionId。3.2 Get Sessions我们可以使用Get Sessions命令查询Sessions的状态，可以通过SessionId获取特定Session状态信息，也可以进行范围查询，获得Sessions列表。通过该命令，我们可以获得Session在Yarn集群对应的application_id、运行状态以及其他状态信息。3.3 Post StatementsPost Statements命令用来向Spark提交代码，一个Statement对应Spark Application中的一个Job。下图中我们提交了一段通过Spark Sql查询Hive表的代码。3.4 Get StatementsStatement创建后，可以使用Get Statements获取Statement的状态信息以及运行结果。下图中返回Body中text/plain中包含了查询Hive表的结果。3.5 Delete SessionsSession闲置一段时间后，Livy会自动删除Session，Spark中对应的Application也会被自动删除。我们也可以通过Delete Sessions来手动删除Session。4. Apache Livy与Spark的逻辑关系上节我们通过Post Sessions创建了一个Session，并使用Get Sessions得到了该Session的状态信息，其中appid为application_1506061012334_0021，这与我们下图中Yarn集群ALL Applications中得到的信息是一致的。上节我们已经提到Livy中每个Session对应Spark中的一个Application，所以创建Session需要指定的参数，与Spark中创建Application时需要指定的参数基本一致。Livy中的Statement的概念对应Spark Application中的Job。Livy中Session包含若干Statement，与Spark中Application包含若干Job一致。同时需要强调的一点是，Livy的Session在闲置一段时间后会自动删除，对应的Spark Application也会自动删除。因此Livy闲置时并不会始终占用Spark资源。","tags":[{"name":"Spark","slug":"Spark","permalink":"http://yoursite.com/tags/Spark/"}]},{"title":"HBase系列5——性能优化","date":"2017-10-15T02:43:26.000Z","path":"2017/10/15/HBase系列5——性能优化/","text":"HBase的性能优化主要围绕提升HBase的读写性能围绕，包括Client端程序优化以及HBase服务端参数配置优化。其中，Client端可以从减少RPC调用次数入手，使用缓存，采用批量请求的方式。HBase服务端参数配置优化，又主要包括良好的RowKey设置保证负载均衡，良好的ColumnFamily设置，适当的MemStore Flush配置和Compaction配置以及合理的内存规划等方面。 1. 读性能优化 1.1 Client端1.1.1 减少RPC请求次数Client端查询程序开发时，针对Get请求，在Client端宕机时允许Get请求丢失一部分的情况下，可以采用批量Get请求的方式，减少RPC请求次数。针对Scan请求，默认会每次从HBase服务端拉取100条数据进行缓存，当Scan的数据量较大时，我们可以增大Caching的条数，每次从HBase服务端拉取1000条数据，从而减少RPC请求次数。 1.1.2 精确查询HBase服务端在读取数据时会Client端的查询条件对表中的数据进行检索，因此Client端在读取数据时尽量指定RowKey或RowKey-Column精确查询，缩小HBase服务端的检索范围，从而提升读取性能。 1.1.3 大数据量Scan关闭缓存Scan拉取的数据都会保存在Client端的本地缓存中，当数据量特别大，由于本地缓存较小，往往会将内存中的其他关键数据交换出去，影响Client端性能。因此针对每条数据仅仅使用一次的情况，我们可以关闭Scan的本地缓存。 1.2 服务端1.2.1 负载均衡HBase中若数据集中在几个甚至一个HRegionServer中，读取数据时，该HRegionServer负载会特别重，成为数据查询的单点性能瓶颈。良好的RowKey设计，可以保证数据均匀的分布在HBase集群中，负载均衡。因此读取数据时，HBase集群可以实现良好的分布式查询。 1.2.2 BloomFilter每个ColumnFamily中都可以配置BloomFilter，分为None/Row/Row-Col三种，当BloomFilter开启时，查询数据的过程中可以利用BloomFilter快速的过滤掉不包含所需数据的HFile文件，可以大大提升读取速度。 1.2.3 BlockCacheBlockCache是读缓存，可以大幅提升读取性能。我们可以针对读多写少的应用场景，适当提高BlockCache的占比，降低MemStore的占比，从而进一步提升读取性能。另一方面，由于BlockCahce中的数据需要根据数据热度，频繁交换，因此BlockCache的垃圾回收机制的配置也至关重要，对BlockCahce的影响较大，在BucketCache的offheap模式下GC表现很优越，建议使用。 1.2.4 Compaction配置Compaction主要用来对HFile文件进行合并，保证Store下的HFile文件不至于过多，影响读取性能。Compaction分为Minor-Compaction和Major-Compaction。 Minor Compaction是指选取一些小的、相邻的StoreFile将他们合并成一个更大的StoreFile，在这个过程中不会处理已经Deleted或Expired的Cell。一次Minor Compaction的结果是更少并且更大的StoreFile。 Major Compaction是指将所有的StoreFile合并成一个StoreFile，这个过程还会清理三类无意义数据：被删除的数据、TTL过期数据、版本号超过设定版本号的数据。另外，一般情况下，Major Compaction时间会持续比较长，整个过程会消耗大量系统资源，对上层业务有比较大的影响。因此线上业务都会将关闭自动触发Major Compaction功能，改为手动在业务低峰期触发。Compaction对HFile的合并主要通过两个参数来设置，hbase.hstore.compactionThreshold表示一个store中的文件数超过多少就应该进行合并。hbase.hstore.compaction.max.size表示参数合并的文件大小最大是多少，超过此大小的文件不能参与合并。hbase.hstore.compactionThreshold设置不能太大，默认是3个；设置需要根据Region大小确定，通常可以简单的认为hbase.hstore.compaction.max.size = RegionSize / hbase.hstore.compactionThreshold1.2.5 ColumnFamily 列簇设计列簇设计优化也是HBase的读写性能优化点，包括列簇数量、BlockSize大小、BloomFilter设置、Compresssion等配置，都会对HBase的读写性能产生影响。1.2.6 HDFS优化因为HBase是基于HDFS存储的，包含HLog和HFile都存储在HDFS上，因此有关HDFS上的一些配置也会影响HBase的性能。 2. 写性能优化 2.1 Client端与读性能优化相同，写性能优化在Client端主要也是尽量减少Client与服务端的RPC通信次数。Client端的Put操作可以单挑写入，也可以通过List同步批量写入，或通过将AutoFlush设置为False，关闭默认的自动提交，这时每个Put请求都会保存在Client端本地缓存，当缓存达到阈值(默认2M)或手动Commit时，才会将缓存的多个Put请求批量发送至HBase服务端，进而实现异步批量操作。需要注意的是，无论时读批量操作还是写批量操作，当Client端宕机时，都有丢失操作请求的风险。 2.2 服务端2.2.1 关闭WALWAL机制下，写入HBase的数据首先会写入HLog进行持久化，然后才会写入MemStore。若部分业务允许HReionServer异常下可以丢失MemStore尚未溢出到HFile的数据，且更加注重HBase的写入性能，则我们可以考虑关闭WAL机制，使写入数据跳过HLog部分，直接写入MemStore，这种方式可以大幅提高HBase的写入性能。 2.2.2 负载均衡与读性能优化小节中提到的相同，良好的RowKey设置，HBase集群良好的负载均衡，可以保证数据的写入性能。另一方面，如果表的Region数量少于HBase集群中HRegionServer数量，将无法充分利用HBase的资源，无法充分进行分布式读写，因此我们可以通过预分区或对高负载Region进行切分等方式，使Region数量不少于HRegionServer数量。 2.2.3 MemStore配置MemStore中的数据在溢出到HFile时以及Store对HFile文件进行Compaction操作时，HBase的写入性能会受到严重影响，因此MemStore中有关Flush和Compaction的参数对HBase的写入性能优化至关重要。 一旦整个RegionServer上所有Memstore占用内存大小总和大于配置文件中upperlimit时，系统就会执行RegionServer级别flush，flush算法会首先按照Region大小进行排序，再按照该顺序依次进行flush，直至总Memstore大小低至lowerlimit。这种flush通常会阻塞HRgionServer较长时间。如果RegionServer上Region较多，而Memstore总大小设置的很小（JVM设置较小或者upper.limit设置较小），就会触发HRegionServer级别flush。因此对于该参数需要合理配置。 hbase.hstore.blockingStoreFiles该参数表示如果当前Store中文件数大于该值，系统将会强制执行Compaction操作进行文件合并，合并的过程会阻塞整个Store的写入。该参数一般建议设置为5～8左右。2.2.4 数据长度过长当HBase的KeyValue保存的数据过长时，写入数据时会耗费较长时间，数据吞吐量也较低，这种情况下，由于HBase耗费很多的资源处理该业务的读写请求，其他业务的读写性能将受到严重影响，这是需要特别注意的一点。 ps: 参考文献及图片来源：有态度的HBase/Spark/BigData","tags":[{"name":"HBase","slug":"HBase","permalink":"http://yoursite.com/tags/HBase/"}]},{"title":"HBase系列4——常用API介绍与Spark读写HBase","date":"2017-10-14T09:34:17.000Z","path":"2017/10/14/HBase系列4——API介绍与Spark读写HBase/","text":"1. Java APIJava API包括了对HBase的各种操作，本节主要对Java API中的基本操作进行简要介绍，诸如批量处理以及过滤器的使用等高级API，感兴趣的可以进一步了解。Java API中的CRUD主要通过HTable类提供的方法实现，而管理和创建HBase表，则通过HBaseAdmin类实现。 1.1 Get 操作Get操作包括单次Get请求和批量Get请求，我们以单次Get请求为例：12345678Configuration conf = HBaseConfiguration.create();HTable table = new HTable(conf, \"testtable\");Get get = new Get(Bytes.toBytes(\"row1\"));get.addColumn(Bytes.toBytes(\"colfam1\"), Bytes.toBytes(\"qual1\"));Result result = table.get(get);byte[] val = result.getValue(Bytes.toBytes(\"colfam1\"), Bytes.toBytes(\"qual1\"));table.close();System.out.println(\"Value: \" + Bytes.toString(val)); 1.2 Put 操作123456Configuration conf = HBaseConfiguration.create();HTable table = new HTable(conf, \"testtable\");Put put = new Put(Bytes.toBytes(\"row1\"));put.add(Bytes.toBytes(\"colfam1\"), Bytes.toBytes(\"qual1\"), Bytes.toBytes(\"val1\"));table.put(put);table.close(); 1.3 Delete 操作123456Configuration conf = HBaseConfiguration.create();HTable table = new HTable(conf, \"testtable\");Delete delete = new Delete(Bytes.toBytes(\"row1\"));delete.deleteColumn(Bytes.toBytes(\"colfam1\"), Bytes.toBytes(\"qual1\"), 1); //删除特定行特定列特定版本table.delete(delete);table.close(); 1.4 Scan1234567891011Configuration conf = HBaseConfiguration.create();HTable table = new HTable(conf, \"testtable\");Scan scan = new Scan();scan.addColumn(Bytes.toBytes(\"colfam1\"), Bytes.toBytes(\"col-5\")).setStartRow(Bytes.toBytes(\"row-10\")).setStopRow(Bytes.toBytes(\"row-20\")); //使用Builder模式精确ScanResultScanner scanner = table.getScanner(scan);for(Result res : scanner) &#123; System.out.println(res);&#125;table.close(); 1.5 HBaseAdmin 操作HBaseAdmin提供了建表、创建列簇、检查表是否存在、修改表结构和列簇结构和删除表等功能。12345678Configuration conf = HBaseConfiguration.create();HBaseAdmin admin = new HBaseAdmin(conf);HTableDescriptor desc = new HTableDescriptor(Bytes.toBytes(testtable));HColumnDescriptor coldef = new HColumnDescriptor(Bytes.toBytes(\"colfam1\"));desc.addFamily(coldef);admin.createTable(desc);boolean avail = admin.isTableAvailable(Bytes.toBytes(\"testtable\"));System.out.println(\"Table available: \" + avail); 2. Shell 基本命令在HBase安装目录下输入./bin/hbase shell即可进入HBase的Shell命令行模式，在模式下可以完成对HBase的一系列操作。 status 查看集群状态信息 version 查看HBase版本信息 create &#39;t1&#39;, {NAME=&gt;&#39;f1&#39;, VERSION=&gt;5} 创建表 alter &#39;t1&#39;, NAME=&gt;&#39;f1&#39;, VERSION=&gt;5 修改表 describe &#39;t1&#39; 获取表的元数据信息和是否可用的状态 disable &#39;t1&#39; 下线表 enable &#39;t1&#39; 上线表 drop &#39;t1&#39; 删除表 exist &#39;t1&#39; 判断某个表是否存在 list 罗列所有表名称 count &#39;t1&#39; 统计表的总行数 delete &#39;t1&#39;, &#39;r1&#39;, &#39;c1&#39;, ts1 删除特点单元格 get &#39;t1&#39;, &#39;r1&#39;, {COLUMN=&gt;{&#39;c1&#39;,&#39;c2&#39;,&#39;c3&#39;}}获取某几行数据 put &#39;t1&#39;, &#39;r1&#39;, &#39;c1&#39;, &#39;value&#39;, ts1 写入数据 scan &#39;t1&#39;, {COLUMNS=&gt;[&#39;c1&#39;,&#39;c2&#39;], LIMIT=&gt;10, STARTROW=&gt;&#39;xyz&#39;} 根据特定条件扫描表 truncate &#39;t1&#39; 清空表以上为Shell经常用到的命令，还有工具命令(compact、flush等)、复制命令等，可以进一步了解。3. Spark读写HBaseSpark操作HBase有两种方式，一种方式是在Spark架构中调用HBase Java API的方式，该方式下要遵循Spark分布式计算的特点来编程。由于HBase提供了对Hadoop MapReduce框架的支持，因此在Spark中我们可以使用另一种方式，即利用NewAPIHadoop接口，实现对HBase的读写。3.1 Spark 写HBase12345678910111213141516171819202122232425262728293031323334353637import org.apache.hadoop.hbase.client.&#123;Put, Result&#125;import org.apache.hadoop.hbase.io.ImmutableBytesWritableimport org.apache.hadoop.hbase.mapreduce.TableOutputFormatimport org.apache.hadoop.hbase.util.Bytesimport org.apache.hadoop.mapreduce.Jobimport org.apache.spark._object HBaseWrite &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setAppName(\"HBaseTest\") sparkConf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") sparkConf.registerKryoClasses(Array(classOf[org.apache.hadoop.hbase.io.ImmutableBytesWritable],classOf[org.apache.hadoop.hbase.client.Result])) val sc = new SparkContext(sparkConf) val tablename = \"test1\" sc.hadoopConfiguration.set(\"hbase.zookeeper.quorum\", \"hadoop001,hadoop002,hadoop003\") sc.hadoopConfiguration.set(\"hbase.zookeeper.property.clientPort\", \"2181\") sc.hadoopConfiguration.set(TableOutputFormat.OUTPUT_TABLE, tablename) val job = new Job(sc.hadoopConfiguration) job.setOutputKeyClass(classOf[ImmutableBytesWritable]) job.setOutputValueClass(classOf[Result]) job.setOutputFormatClass(classOf[TableOutputFormat[ImmutableBytesWritable]]) val indataRDD = sc.makeRDD(Array(\"1,jack,15\", \"2,Lily,16\", \"3,mike,16\")) val rdd = indataRDD.map(_.split(',')).map &#123; arr =&gt; &#123; val put = new Put(Bytes.toBytes(arr(0))) put.addColumn(Bytes.toBytes(\"cf1\"), Bytes.toBytes(\"name\"), Bytes.toBytes(arr(1))) put.addColumn(Bytes.toBytes(\"cf1\"), Bytes.toBytes(\"age\"), Bytes.toBytes(arr(2).toInt)) (new ImmutableBytesWritable, put) &#125; &#125; rdd.saveAsNewAPIHadoopDataset(job.getConfiguration) sc.stop(); &#125;&#125; 3.2 Spark 读HBase12345678910111213141516171819202122232425262728293031323334353637383940414243444546import org.apache.hadoop.hbase.client.HBaseAdminimport org.apache.hadoop.hbase.mapreduce.TableInputFormatimport org.apache.hadoop.hbase.util.Bytesimport org.apache.hadoop.hbase.&#123;HBaseConfiguration, HTableDescriptor, TableName&#125;import org.apache.spark._object HBaseRead &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setAppName(\"HBaseTest\") sparkConf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") sparkConf.registerKryoClasses(Array(classOf[org.apache.hadoop.hbase.io.ImmutableBytesWritable],classOf[org.apache.hadoop.hbase.client.Result])) val sc = new SparkContext(sparkConf) val tablename = \"test1\" val conf = HBaseConfiguration.create() conf.set(\"hbase.zookeeper.quorum\", \"hadoop001,hadoop002,hadoop003\") conf.set(\"hbase.zookeeper.property.clientPort\", \"2181\") conf.set(TableInputFormat.INPUT_TABLE, tablename) // 如果表不存在则创建表 val admin = new HBaseAdmin(conf) if (!admin.isTableAvailable(tablename)) &#123; val tableDesc = new HTableDescriptor(TableName.valueOf(tablename)) admin.createTable(tableDesc) &#125; //读取数据并转化成rdd val hBaseRDD = sc.newAPIHadoopRDD(conf, classOf[TableInputFormat], classOf[org.apache.hadoop.hbase.io.ImmutableBytesWritable], classOf[org.apache.hadoop.hbase.client.Result]) val count = hBaseRDD.count() println(count) hBaseRDD.collect.foreach &#123; case (_, result) =&gt; &#123; //获取行键 val key = Bytes.toString(result.getRow) //通过列族和列名获取列 val name = Bytes.toString(result.getValue(\"cf1\".getBytes, \"name\".getBytes)) val age = Bytes.toInt(result.getValue(\"cf1\".getBytes, \"age\".getBytes)) println(\"Row key:\" + key + \" Name:\" + name + \" Age:\" + age) &#125; &#125; admin.close() sc.stop() &#125;&#125; 3.3 Spark Scan HBase1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import org.apache.hadoop.hbase.HBaseConfigurationimport org.apache.hadoop.hbase.client.Scanimport org.apache.hadoop.hbase.io.ImmutableBytesWritableimport org.apache.hadoop.hbase.protobuf.ProtobufUtilimport org.apache.hadoop.hbase.protobuf.generated.ClientProtosimport org.apache.hadoop.hbase.util.&#123;Base64, Bytes&#125;import org.apache.spark.&#123;SparkConf, SparkContext&#125;object HBaseScan &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setAppName(\"HBaseTest\") sparkConf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") sparkConf.registerKryoClasses(Array(classOf[org.apache.hadoop.hbase.io.ImmutableBytesWritable], classOf[org.apache.hadoop.hbase.client.Result])) val sc = new SparkContext(sparkConf) val tablename = \"test1\" val conf = HBaseConfiguration.create() val scan = new Scan() scan.setStartRow(Bytes.toBytes(args(0))) scan.setStopRow(Bytes.toBytes(args(1))) scan.addColumn(Bytes.toBytes(\"cf1\"), Bytes.toBytes(\"name\")) def convertScanToString(scan: Scan) = &#123; val proto: ClientProtos.Scan = ProtobufUtil.toScan(scan) Base64.encodeBytes(proto.toByteArray) &#125; conf.set(\"hbase.zookeeper.quorum\", \"hadoop001,hadoop002,hadoop003\") conf.set(\"hbase.zookeeper.property.clientPort\", \"2181\") /** TableInputFormat 中有若干参数可以用来过滤 ,可以参考看一下TableInputFormat的静态常量 */ conf.set(org.apache.hadoop.hbase.mapreduce.TableInputFormat.SCAN, convertScanToString(scan)) conf.set(org.apache.hadoop.hbase.mapreduce.TableInputFormat.INPUT_TABLE, tablename) val rdd = sc.newAPIHadoopRDD(conf, classOf[org.apache.hadoop.hbase.mapreduce.TableInputFormat], classOf[ImmutableBytesWritable], classOf[org.apache.hadoop.hbase.client.Result]) rdd.collect.foreach &#123; case (_, result) =&gt; &#123; //获取行键 val key = Bytes.toString(result.getRow) //通过列族和列名获取列 val name = Bytes.toString(result.getValue(\"cf1\".getBytes, \"name\".getBytes)) // val age = Bytes.toInt(result.getValue(\"cf1\".getBytes, \"age\".getBytes)) println(\"Row key:\" + key + \" Name:\" + name ) &#125; &#125; sc.stop() &#125;&#125;","tags":[{"name":"HBase","slug":"HBase","permalink":"http://yoursite.com/tags/HBase/"}]},{"title":"HBase系列3——表结构设计","date":"2017-10-13T05:36:54.000Z","path":"2017/10/13/HBase系列3——表结构设计/","text":"1. 表结构设计至关重要在使用HBase的过程中，HBase表结构设计的优劣对读写性能有着较大影响。由于HBase只能对RowKey进行索引，因此若RowKey没有针对具体需求做特殊设计，Client端在Get/Scan数据表时，往往需要遍历Row中的所有数据列，并做一些较为复杂的逻辑处理，才能得到所需数据，读写性能较差。RowKey的设计还需要考虑到HBase的RowKey采用从小到大字典排序的特点，且RowKey相同的数据会在同一个HRegionServer中，当RowKey相似或存在数据热点时，大量的数据会存储在同一个HRegionServer中，对数据读写时会存在单点性能瓶颈，无法发挥HBase分布式存储的优势。ColumnFamily数量、数据的压缩方式、数据版本数、数据时间周期TTL以及二级索引表等，对HBase的性能也有着重要影响，这些参数在进行HBase表结构设计时，都需要着重考虑，只有全面考虑才能最大发挥HBase的性能优势。 2. 建表示例create &#39;TestTable&#39;, {NAME=&gt;&#39;CF1&#39;, BLOOMFILTER=&gt;&#39;ROW&#39;, BLOCKSIZE=&gt;&#39;65536&#39;, VERSIONS=&gt;&#39;3&#39;, COMPRESSION=&gt;&#39;SNAPPY&#39;, BLOCKCACHE=&gt;&#39;true&#39;, IN_MEMORY=&gt;&#39;false&#39;, TTL=&gt;&#39;3600&#39;}以上建表语句，表示建立一个名为TestTable的表，只有一个名为CF1的列簇，该列簇中BLOOMFILTER为ROW方式，HFile数据块大小为65536B(64KB)，版本数为3，数据采用SNAPPY的压缩方式，BLOCKCACHE为true(默认)，IN_MEMORY为false(默认)， 数据存储周期为3600s。建表语句中具体参数含义会下面会具体解释。 3. RowKey设计3.1 排序问题HBase只支持基于RowKey的索引，因此RowKey的设计决定了HBase的读写性能。HBase将不同的RowKey，按照范围StartKey~EndKey存储在不同的Region中，且HBase对RowKey按从小到大字典排序。具体应用中RowKey一般为单个关键数据或多个关键数据拼接而成，若RowKey中包含时间戳，建议时间戳采用将Long类型转换为Bytes的方式，若索引数据时要按照时间从新到旧的数据，则建议对RowKey中的时间戳进行(Integer.MAX_VALUE-timestamp）处理，实现RowKey按照时间从新到旧排序。 3.2 数据聚集的负载均衡问题HBase中若出现数据聚集的情况，即数据大多集中在个别RowKey中，则会出现HBase只对个别HRegionServer进行读写访问，负载较重，而集群中其他HRegionServer处于闲置状态，造成HBase集群读写性能差的问题。为了防止这种问题，在设计RowKey时需要考虑负责均衡，将RowKey尽量均匀的分布在HBase集群中。这主要通过将字符串的Hash值或字符串的MD5值等作为RowKey来实现。比如对UID取Hash值，或使用UID的MD5值前6位等。从而将RowKey均为分布到HBase集群中。 3.3 二级索引实际应用中只根据RowKey进行数据索引有时候无法满足要求，往往需要对数据进行二次索引，二级索引的实现有多种方式，这里我们简单讲一下使用Client-Managed方式，即由Client维护二级索引。假设存在一张表table1，其中RowKey为uid，ColumnFamily为attr，数据为用户的基本属性province、age、gender等。具体需求除了根据uid查找用户属性外，还需要根据省份、年龄等统计用户的省份分布、年龄分布等。针对第二个需求，如果没有二级索引，我们需要遍历整张table1表，提取每行的省份、年龄等数据，并进行统计，一般使用MapReduce完成，整个统计过程较慢。另一种方式，我们可以建立二级索引来实现，根据属性表，采用反向索引，建立索引表table2，其中RowKey为province_age，ColumnFamily为user，Column为具体uid，Value为1。当我们想要统计辽宁省20岁的用户时，我们可以根据RowKey:辽宁省_20，在table2中索引，得到该行中user列簇下的所有列，即为辽宁省20岁的所有用户uid列表。同时，我们还可以根据结果中的特定uid，在用户属性表table1中查询该uid的具体用户属性，完成二级索引的过程。这种二级索引的方式，可以大大加快数据统计的过程。 4. ColumnFamily设计4.1 列簇长度在HFile中DataBlock的KeyValue结构中除了保存Value值之外，还保存有RowKey、ColumnFamily和ColumnQuantifier,因此在设计RowKey和ColumnFamily的长度在保证可读性的前提下要尽量做到最短，特别是ColumnFamily的长度要尽可能的短，如’cf’等。 4.2 列簇数量ColumnFamily的数量建议是越少越好，建议1至2个，3个及3个以上HBase的性能会出现下降。因为每个Region中的每个Store都对应一个ColumnFamily。而MemStore的flush溢出磁盘操作和对HFile的Compaction合并操作是基于Region来操作的。即若设置为5个ColumFamily，Region中会出现5个Store，其中任何一个Store的MemStore满，都会导致所有5个Store一起进行flush操作。任何一个Sotre中的HFile数量满足Compaction条件，所有5个Store的HFile都会进行Compaction操作。另一方面，假如表Table1有两个ColumnFamily，分别为CF1和CF2，其中CF1的有100万行，CF2有1亿行，则受CF2数量大的影响，CF1中的数据也会分布在多个Region中，因此当对CF1进行Scan操作时，性能会很差。因此在进行表结构设计时，ColumnFamily的数量越少越好，1个最佳。 4.3 BLOCKSIZEBlockSize决定了HFile中DataBlock数据块的大小，默认64KB，实际使用中需要根据具体应用场景进行设置。DataBlock越小，IndexBlock就越大，加载HFile时会占用更多的内存空间，但其随机查找性能会更好。若应用场景中多为顺序Scan操作，则一次读取更多的数据到内存中更为合理，这时DataBlock要设大一些。 4.4 VERSIONSHBase中每个单元格都有版本号，多以时间戳作为版本号，默认情况下，每个单元格只有3个时间版本。HBase中不存在真正的更新操作，更新操作实际上是对单元格增加一个新的版本。如果我们只需要一个版本，可以将列簇的VERSIONS设为1。 4.5 COMPRESSIONHFile可以压缩后存放在HDFS中，节约存储空间，但是以牺牲CPU和读写时间为代价的，因此我们需要选取压缩比高且压缩、解压缩速度快的压缩方式。HBase支持LZO、SNAPPY和GZIP等压缩方式，建议使用SNAPPY方式。 4.6 BLOCKCACHEBlockcache为读缓存，默认开启，当应用场景中很少进行读数据或多为顺序读时，可以考虑关闭BlockCache。当应用场景为随机读数据时，BlockCache可以将热点数据放入BlockCache中，当再读取该数据时可以避免从HFile中查询，而是从BlockCache中直接读取，大大加快了读取速度。 4.7 BLOOMFILTERBloomFilter，分为None/Row/Row-Col三种，当BloomFilter开启时，查询数据的过程中可以利用BloomFilter快速的过滤掉不包含所需数据的HFile文件，可以大大提升读取速度。当数据查询多以Row为索引时，BloomFilter可以设置为Row，当数据查询多以Row-Column为索引时，BloomFilter可以设置为Row-Col，这时HFile占用的存储空间会大一些。因为BloomFilter的信息是保存在HFile文件中的。建议开启BLOOMFILTER。 4.8 TTLTTL为生成时间，即单元格中数据的保存时间，单位为s，当TTL设为18000s，列簇中超过18000s的数据将会在下次Major-Compaction时被删除。 4.9 IN_MEMORYIN_MEMORY默认为false，当设为true时，即指定该列簇中的数据保存在内存中，而非HFile中。该配置一般适用于表特别小且频繁查询的场景，比如可以考虑将元数据表的列簇IN_MEMORY设为ture。","tags":[{"name":"HBase","slug":"HBase","permalink":"http://yoursite.com/tags/HBase/"}]},{"title":"HBase系列2——读写流程","date":"2017-10-12T07:06:35.000Z","path":"2017/10/12/HBase系列2——读写流程/","text":"1. HBase写入流程 HBase写入流程主要分为四部分，分别为Client端写入、写入HLog日志、写入MemStore、MemStore溢出到HFile落地磁盘 1.1 Client端用户写入HBase数据时，首先会通过HBase提供的API接口，调用Put类，将数据写入本地缓存，若用户设置的AutoFlush=Ture(默认)，则每调用一次Put操作，Client会将请求立即发送给HBase集群；若用户设置的AutoFlush=False，则每调用一次Put操作，Client会将请求缓存至本地缓存，当缓存达到阈值时(默认2M)，会批量向HBase集群提交Put请求，也可以调优flushcommit()方法强制将缓存中的put请求提交至HBase集群。两种提交方式相比，AutoFlush=False的批量操作数据吞吐量更高，但是由于Put请求都缓存在本地缓存，因此存在数据丢失的风险。上述提及的Put批量提交是通过关闭AutoFlush机制实现的，与使用List这种方法异曲同工，List将多个Put操作放入List中，通过提交List的方法也可以实现批量写入。 1.2 HLogHBase为了保证数据不会丢失，采用了Write-Ahead-Logging(WAL)机制。HRegionServer接收到数据后，首先将数据顺序写入(Append)到HLog文件(每个HRegionServer对应一个HLog文件)，HLog文件中每条数据存储格式为LogSeqNum-WriteTime-ClusterIds-RegionName-TableName。HLog文件在HDFS中存储，在HRegionServer宕机情况时，HMaster将该HRegionServer对应的Region分配至其他正常HRegionServer中，并从HDFS中读取HLog文件，将宕机时丢失的数据在新节点中恢复。HLog文件不会越来越大，因为当MemStore缓存的数据溢出到HFile文件时，会将HLog文件中对应的数据删除，MemStore与HLog的数据是通过SequenceId实现关联的。HBase中可以通过设置WAL的持久化等级决定是否开启WAL机制、以及HLog的落盘方式。 SKIP_WAL：只写缓存，不写HLog日志。 ASYNC_WAL：异步将数据写入HLog日志中。 SYNC_WAL(默认方式)：同步将数据写入日志文件中，数据只是被写入文件系统中，并没有真正落盘。 FSYNC_WAL：同步将数据写入日志文件并强制落盘。最严格的日志写入等级，可以保证数据不会丢失，但是性能相对比较差。1.3 MemStore在HRegionServer的Region中每个Store存在一个MemStore，HBase的写入性能与MemStore密切相关，数据在写入HLog后，会再写入MemStore中，当达到溢出条件时，MemStore中的数据会溢出成HFile文件存入HDFS中。需要注意的是，MemStore溢出时是Region中的所有Store都会溢出。这也是为什么建议ColumnFamily数量少一些的原因，若存在cf1、cf2、cf3多个ColumnFamily，且cf3中频繁写入数据，则当cf3对应的Store中的MemStore满时，cf1、cf2、cf3三个对应的MemStore都要进行溢出操作，性能较差。2. HBase读取流程HBase的存储基于LSM树(Log-Structured Merge Tree)结构，一次范围查询可能会涉及多个分片、多块缓存甚至多个数据存储文件，同时HBase中更新操作以及删除操作实现都很简单，更新操作并没有更新原有数据，而是直接写入数据并更新版本。删除操作也并没有真正删除原有数据，只是插入了一条打上”deleted”标签的数据，而真正的数据删除发生在系统异步执行Major_Compact的时候。这种实现大大简化了数据更新、删除流程，但是对于数据读取来说则需要根据版本进行过滤，同时对已经标记删除的数据也要进行过滤，同时，Major_Compact对HFile合并并删除数据的过程成为HBase中对性能影响较大的操作。2.1 Client端 Client端读取数据时首先从Zookeeper中获取存储-META-表的HRegionServer地址，然后从该HRegionServer中读取-META-表并缓存在本地缓存中。(0.9.5版本后的HBase已经取消掉-ROOT-表）。 Client根据RowKey从本地缓存的-META-表查询，获得存储该RowKey数据的HRegionServer地址，并向该HRegionServer发送读请求。 当HBase更新-MEAT-表时，Client根据缓存的旧-MEAT-表查询数据会时会报异常，这时客户端会重新从Zookeeper中拉取新的-META-表。 Client向HBase写入数据时，也需要向读取数据时一样，从Zookeeper读取-META-表读取到本地缓存，并基于缓存到本地的-META-表，获取HRegionServer地址，并向该HRegionServer发送数据写入请求。 由该流程可知，Client端读写数据时只会与Zookeeper和HRegionServer进行交互，HMaster不会参与，因此HMaster负载较轻，且HMaster故障时不会影响HBase的读写，不会称为性能瓶颈点和故障点。2.2 HRegionServer服务端 服务端数据查询顺序为：BlockCache -&gt; MemStore -&gt; HFile HRegionServer在接收到Client发送来的Scan/Get操作后，首先会在每个Region中构建一个RegionScanner，RegionScanner会为Region下的每个列族，即每个Store构建一个StoreScanner。每个StoreScanner会为Store下的每个HFile构建一个StoreFileScanner，并为MemStore构建MemStoreScanner。 StoreFileScanner与MemStoreScanner根据RowKey过滤掉无效的Scanner，这个过程会用到BloomFilter，因此BloomFilter对HBase的查询性能有较大影响。 有效的StoreFileScanner根据RowKey首先在BlockCache中查找数据，若BlockCache中没有，则在MemStore中查找，若依然没有，最后才会在HFile中根据二分查找法查找数据。 将该Store中所有StoreFileScanner和MemStoreScanner查找到的数据按从小到大排序构建最小堆。 将所有的StoreScanner查找到的结构按小到大排序合并构建最小堆。 2.3 HFile索引机制HFile文件结构如下图所示： StoreFileScanner对HFile文件进行索引时，首先根据RowKey从HRegionServer的BlockCache中查找，若BlockCache中存在查找的数据，则直接读取；若BlockCache中没有所需数据，则从HFile中查找。首先会查询HFile的IndexBlock，从IndexBlock中查询RowKey所对应的DataBlock。为了提高查询速度，IndexBlock分为三级索引结构，即RootIndexBlock、IntermediateIndexBlock和LeafIndexBlock。经过索引三层索引获得DataBlock的所在位置，然后在DataBlock中查询RowKey所对应的数据。HFile索引过程如下图所示： ps: 参考文献及图片来源：有态度的HBase/Spark/BigData","tags":[{"name":"HBase","slug":"HBase","permalink":"http://yoursite.com/tags/HBase/"}]},{"title":"《SQL学习指南》阅读总结","date":"2017-10-09T07:00:16.000Z","path":"2017/10/09/《SQL学习指南》阅读总结/","text":"1. 写在前面SQL语句在工作中经常使用，之前阅读过《SQL必知必会》，但并没有系统的学习过SQL，国庆8天假期，借此机会系统的阅读了《SQL学习指南》一书。越来越发现每次阅读完一本书，如果能够好好的总结，用自己的语言复述一遍书中的主要知识点，往往能够得到更多收获。由于本文完全是我对着该书目录，按照自己的记忆进行的简单知识总结，所以文中知识点并不全面和严谨，仅供参考。 2. 阅读总结SQL语言在多种数据库中都得到了广泛应用，无论是MYSQL、Orcale、SQL Server等关系型数据库，还是Hive、HBase等大数据组件，都可以使用SQL语言进行数据的查询等操作(HBase需要通过Phoenix来进行SQL查询，Phoenix是构建在HBase上的一个SQL层）。同时，不仅是数据库管理人员，程序开发人员、数据统计/挖掘人员也需要掌握一定的SQL。SQL语言主要包括实现对数据库的CRUD操作。 2.1 MySQL数据类型2.1.1 字符数据类型字符型数据主要为char(n)和varchar(n)，两种，char为定长类型，n为数据长度，当数据长度不够长时，向右使用空格补足。vachar可以做到变长，n为最大数据长度。因为varchar最大可以做到65535，所以一般情况下varchar即可满足字符类型的大部分需要，对于文本数据，则需要使用text类型。 2.1.2 数值类型数值类型主要包括float、double、int等，而int由分为tinyint、smallint、smallint、int、mediumint、longint等，大小范围不同，当数据为无符号类型时，可以添加unsigned修饰，大小范围可扩大一倍。 2.1.3 时间类型时间类型主要包括date(YYYY-MM-DD)、datetime(YYYY-MM-DD hh:mm:ss)、timestamp(YYYY-MM-DD hh:mm:ss)等几种，其中datetime范围比timestamp大，且timestamp的最大范围到2037年，datetime的最大范围到9999年，这是需要注意的一点。 2.2 MySQL数据操作2.2.1 常用操作代码以《SQL学习指南》书中提供的bank数据库为例。 mysql -u root -p 进入mysql命令行模式，user为root, 默认password为空 show databases 查看数据库列表 use bank 进入bank数据库 show tables; 可以查看bank数据库下的存在的数据表列表 desc account; 可以查看account表的表结构 CREATE TABLE test (id int auto_increment, name varchar(30) NOT NULL, city varchar(20), PRIMARY KEY id_primary_key (id)); 可以创建一个名为test的表，其中id为int类型，自增， name为varchar(30)类型，不能为NULL， city为varchar(20)类型，默认可以为NULL，并指定id为主键，主键不能为NULL DROP TABLE test; 可以删除test表 INSERT INTO test (id, name, city) VALUES (null, &#39;Albert Lee&#39;, &#39;Beijing&#39;; 可以向test表中添加数据，其中id可以省略 UPDATE test SET name=&#39;Devin Lee&#39;, city=&#39;Shenzhen&#39; WHERE id=1;可以修改test表中id=1的行，将name和city都修改为指定数据，进行UPDATE操作时务必添加WHERE条件，指定需要修改的行，否则会对表中所有行进行UPDATE操作 DELETE FROM test WHERE id=1; 可以删除id=1的行 SELECT * FROM test; 可以查询test表，也可以指定所要查询的特定列，也可以使用WHERE添加过滤信息 ALTER TABLE test DROP COLUMN city;删除city列 ALTER TABLE test CHANGE name name2 varchar(50) not null;将name列修改为name2，且类型更改为varchar(50) ALTER TABLE test ADD post_code vahrchar(20) not null default &#39;0&#39;;添加post_code列，不允许为NULL，且默认值为’0’2.2.2 关于字符串函数与时间函数concat函数可以在select中使用对字符串进行拼接，like函数可以在where中使用实现模糊查询，其中%代表任意个(包括0个)字符，_ 表示一个字符。时间函数较多，包括将字符串转换为时间类型，提取年月日，对时间类型增加指定时间等操作。UNIX_TIMESTAMP(&#39;1999-10-10&#39;) 将日期转换为UNIX时间戳，FROM_UNIXTIME(875996580) 将UNIX时间戳转换为日期。2.2.3 非关联子查询与关联子查询非关联子查询与关联子查询是非常重要的概念。两者之间的区别在于，非关联子查询独立于外部表，在查询时首先运行子查询，当非关联子查询整体都运行完，才运行外部查询。关联子查询与外部表相关联，关联子查询是对外部查询的每一行都运行一次关联子查询。1select * from account where open_branch_id in (select branch_id from branch where name like ('W%')); 该语句即为一条包含非关联子查询的SQL语句，首先运行select branch_id from branch where name like (&#39;W%&#39;)从branch表中查询出name以W开头的branch_id，然后 在account中使用where open_branch_id in过滤条件查询出account表中open_branch_cd符合子查询结果的行。该语句首先运行子查询，带子查询整体运行完，使用查询出的结果集作为外部的过滤集，再进行外部查询。因此该语句的子查询属于非关联子查询。1select a.account_id, (select b.name from branch b where a.open_branch_id = b.branch_id) branch_name from account a; account表共24行数据，该语句对account表的每一行都会运行一次select b.name from branch b where a.open_branch_id = b.branch_id子查询且子查询中where的过滤条件用到了外部表account，因此该语句中的子查询属于关联子查询。 2.2.4 关于连接SQL中经常需要使用多个表进行联合查询，这种情况下就需要用到连接操作JOIN。常用的JOIN分为inner join、left outer join、right outer join以及accross join。1select a.account_id, b.name from account inner join branch b on a.open_branch_id=b.branch_id; inner join内连接为默认连接，该语句即为一个内连接操作，内连接根据连接条件，将两个表的数据连接在一起，假如account表中的某行数据open_branch_id在branch表中没有对应的branch_id或branch表中某行数据的branch_id在account表中没有对应的open_branch_id，则该行不会在结果集中出现。即inner join内连接的结果集只会包含在两个表中都存在对应的数据。left outer join与right outer join相对应。1select a.account_id, b.name from account left outer join branch b on a.open_branch_id=b.branch_id; 该语句作为左外连接，结果集中以join左边的account表为准，包含account表的所有行，若account表的某行在branch表中没有与之对应的，则结果集中该行的b.name设为NULL。1select a.account_id, b.name from account right outer join branch b on a.open_branch_id=b.branch_id; 该语句作为右外连接，结果集中以join右边的branch表为准，包含branch表的所有行，若branch表的某行在account表中没有与之对应的，则结果集中该行的a.account_id设为NULL。left outer join 与right outer join是可以相互转换的，只需要调换两表的位置即可。 2.2.5 关于分组与HAVING在SQL语句中，我们经常需要对查询的数据按照特定列进行分组，求数据的MAX()最大值、MIN()最小值、AVG()平均值、COUNT()行数以及SUM()求和等操作，因此需要GROUP BY()命令，同时也可以对结果集按照分组后的结果进行过滤，比如SUM()&gt;100的过滤条件，这种情况下，WHERE语句是无法满足的，因为WHERE的过滤条件只能是分组之前的，针对分组之后的过滤操作，需要使用HAVING命令。所以在SQL语句中GROUP BY与HAVING命令在WHERE之后。1select open_branch_id, sum(avail_balance) abc from account where open_date &gt; '2004-06-01' group by open_branch_id having abc&gt;10000; 该语句中对account表首先使用where过滤出open_date &gt; ‘2004-06-01’的行，并使用group by 对这些行按照open_branch_id分组对avail_balance求和，然后使用having过滤出sum(avail_balance)&gt;10000的行，得出最后的结果集。 2.2.6 关于索引与约束当我们使用SQL查询特定数据时，比如where branch_id =2这种过滤条件，MySQL服务器如果扫描branch表的所有行来查找，性能会很差。这时候就需要用到INDEX索引，我们可以在branch表中对branch_id列建索引，索引相当于对branch_id建了另一张表，记录了每个branch_id在branch表中对应的行位置，有了这个索引，当根据branch_id查找特定行时，不需要扫描branch整张表，而是根据索引查找branch对应的行，性能会好很多。我们可以在create创建表时在表中添加索引，索引分为normal普通索引、unique唯一索引和full text文本索引，也可以使用ALTER branch ADD INDEX branch_index (branch_id);在branch表中为branch_id列添加索引。SHOW INDEX FROM branch;SQL中的约束包括唯一约束、外键约束、检查约束等，约束使用CONSTRAINT关键字修饰，KEY相当于索引约束，在创建PRIMARY KEY主键时自动创建唯一约束和唯一索引，创建FOREIGN KEY外键时需要使用REFERENCES指定关联的外部表和列，并自动创建外键约束和普通索引。 2.2.7 关于锁与事务 MySQL根据存储引擎的不同锁机制也不相同，有表锁、页面锁、行锁等，保证了并发性。 在 MySQL 中只有使用了 Innodb 数据库引擎的数据库或表才支持事务。 事务处理可以用来维护数据库的完整性，保证成批的SQL语句要么全部执行，要么全部不执行。 事务用来管理 insert,update,delete 语句 事务是必须满足4个条件（ACID）： Atomicity（原子性）、Consistency（稳定性）、Isolation（隔离性）、Durability（可靠性） 事务的原子性：一组事务，要么成功；要么撤回。 稳定性 ：有非法数据（外键约束之类），事务撤回。 隔离性：事务独立运行。一个事务处理后的结果，影响了其他事务，那么其他事务会撤回。事务的100%隔离，需要牺牲速度。 可靠性：软、硬件崩溃后，InnoDB数据表驱动会利用日志文件重构修改。可靠性和高速度不可兼得 BEGIN TRANSACTION 开始事务 COMMIT 提交事务 ROLLBACK 回滚 SAVEPOINT 设置还原点 ROLLBACK TO a 回滚至a还原点 事务分为自动提交和手动提交，可以进行设置2.2.8 关于视图SQL中的视图主要用途为隐藏敏感信息或向用户提供有限的数据。1CREATE VIEW customer_vw (cust_id, fed_id, city) AS (SELECT cust_id, SUBSTRING(fed_id, 8, 4), city FROM customer); 该语句创建了customer_vw视图，向用户提供了customer表的cust_id、SUBSTRING(fed_id, 8, 4)、city列，隐藏了fed_id列的敏感信息。然后我们可以像查询正常表一样，查询customer_vw视图了，需要注意的是向视图中使用INSERT/UPDATE操作时，并不是所有视图都可以，需要根据视图的创建语句而定。","tags":[{"name":"SQL","slug":"SQL","permalink":"http://yoursite.com/tags/SQL/"}]},{"title":"Spark杂谈1","date":"2017-10-09T03:30:16.000Z","path":"2017/10/09/Spark杂谈1/","text":"1. Spark为什么比Hadoop快Spark比Hadoop快，由于Spark可以将中间结果缓存在内存中，避免频繁读取磁盘，因此在机器学习等需要对数据进行多次叠代计算的场景中，Spark比Hadoop快得多。然而即使在不使用内存缓存的场景中，Spark依然比Hadoop快，具体原因主要在以下方面： 1.1 进程与线程Spark与Hadoop都会将程序分解成Task在节点运行，但Hadoop中的Task为进程，Spark中的Task为线程，因此在Task调度方面，Spark更轻更快。 1.2 编程语言Spark原生支持Scala语言，Hadoop采用Java语言。Scala语言是一种函数式语言，函数式编程语言不需要考虑死锁，因为它不修改变量，所以根本不存在”锁”线程的问题。不必担心一个线程的数据，被另一个线程修改，所以可以很放心地把工作分摊到多个线程，实现并发编程。因此，Scala的并行性明显优于面向对象的Java语言。 1.3 ShuffleSpark的shuffle和Hadoop有所不同。Hadoop MapReduce 将处理流程划分为：map, spill, merge, shuffle, sort, reduce等阶段，shuffle是位于map和reduce中间的一个阶段。在Spark中，没有这样功能明确的阶段。Spark将用户定义的计算过程转化为一个被称作Job逻辑执行图的有向无环图（DAG），图中的顶点代表RDD，边代表RDD之间的依赖关系。再将这个逻辑执行图转化为物理执行图，具体方法是：从逻辑图后往前推算，遇到 ShuffleDependency 就断开，最后根据断开的次数n，将其化分为（n+1）个stage。每个 stage 里面 task 的数目由该 stage 最后一个 RDD 中的 partition 个数决定。因此，Spark的Job的shuffle数是不固定的。Spark只有在shuffle的时候才会将数据放在磁盘，而MR却不是。 1.4 工作流典型的MR工作流是由很多MR作业组成的，他们之间的数据交互需要把数据持久化到磁盘才可以；而Spark支持DAG以及pipelining，在没有遇到shuffle完全可以不把数据缓存到磁盘。 1.5 缓存虽然目前HDFS也支持缓存，但是一般来说，Spark的缓存功能更加高效，特别是在SparkSQL中，我们可以将数据以列式的形式储存在内存中。 2. Spark为什么采用延迟计算 Spark使用惰性求值可以把一些操作合并到一起来减少计算数据的步骤。在类似 Hadoop MapReduce的系统中，开发者常常花费大量时间考虑如何把操作组合到一起，以减少MapReduce的周期数。 在Spark中，写出一个非常复杂的映射并不见得能比使用很多简单的连续操作获得好很多的性能。因此，用户可以用更小的操作来组织他们的程序，这样也使这些操作更容易管理。","tags":[{"name":"Spark","slug":"Spark","permalink":"http://yoursite.com/tags/Spark/"}]},{"title":"HBase系列1——简介及基本架构","date":"2017-08-27T03:12:13.000Z","path":"2017/08/27/HBase系列1——简介及基本架构/","text":"1. 简介HBase是大数据组件中No-SQL数据库的典型代表，提供了高可靠、可伸缩、高性能、面向列的分布式存储。HBase逻辑上引入了Row-ColumnFamily-Qualifier(限定符)-Value-TimeStamp(Version)的概念，Value在数据库中以Byte[]数组存在，且Vaule允许为空。与关系型数据库相比，HBase相当于一个巨大的松散嵌套Map，因此对HBase进行横向扩展是极其方便的，可在廉价物理机上搭建并根据数据量的增多而横向扩展，为海量数据提供高性能的可靠存储。HBase的数据写入首先将数据缓存在内存中，达到阈值后再顺序刷写磁盘的方式(所有的Write/Update均为Append磁盘文件的方式），避免了随机写入造成的性能低下，同时HBase以HDFS为最终的存储层，利用HDFS的特性保证了数据的容灾性。HBase的数据读取提供了服务端缓存、BloomFilter(布隆过滤)等机制，保证了随机读取的性能。 2. HBase与RDBMS的对比HBase作为NoSql数据库，采用分布式存储在读写速度方面比RDBMS数据库快，且横向扩展能力强，当存储空间和读写速度遇到瓶颈时，通过增添机器横向扩建，存储空间和读写速度都能够得到很大提升。同时HBase采用列式存储，且结构松散，允许某列为空，可以存储半结构化数据。但这些都是有代价的，是以牺牲事务为代价的，且RDBMS支持多种数据类型，可以对任意列建立索引，支持事务，支持SQL语言，可以在多个表之间进行Join等操作，只需要一条SQL语句即可，同时支持表锁、页锁与行锁等。与之相比，HBase仅支持字节数组数据结构，仅支持RowKey索引，不支持多表Join，原生不支持SQL语言，需要通过外部插件才能实现对SQL语言的支持。因此在选择时，需要根据具体的业务需要，从多方面综合考虑来选取最终的存储方案。 3. HBase架构HBase架构中，整体上分为Clien——Zookeeper——Master——RegionServer四部分，而RegionServer内部又分为Region——HLog——BlockCache(上图中未标出)，Region中又根据ColumnFamily分为多个Store(每个ColumnFaily对应一个Store)，Store中包含MemStore和多个StoreFile，每个StoreFile中存在一个HFile，HFile存在HDFS中，最终的数据即在HFile中。 3.1 Client创建Client时只需要提供Zookeeper地址即可(为了保证高可用，通常提供Zookeeper的物理机地址不只一个)，Client与Zookeeper通信后可获得HBase集群中的其他物理机地址。Client通过HBase提供的HTable、Put、Get、Filter、BloomFilter、HBaseAdmin等Java类接口(HBase也提供了其他编程语言接口以及RestFul API)，可以实现对HBase的建表、增删改查，以及客户端过滤Filter和服务端过滤BloomFilter（可进一步减少查询结果数据量，减少网络通信和磁盘IO）。同时Client中的一些相应配置(Baching配置等)与HBase的读写性能密切相关，在Client端程序需要综合考虑多种配置，选用最优配置，调用最优接口，从而保证HBase的读写性能。 3.2 ZookeeperZookeeper在HBase中扮演了极其重要的角色： 为了保证集群的高可用，减轻单点故障对集群造成的影响，HBase集群中Maser一般为多台，但只有一台为Active Maser，当Master宕机时，Zookeeper会根据选举算法重新选举出另一台Master。 HBase集群中每台RegionServer周期性的向Zookeeper发送心跳包，当某台RegionServer宕机时，Zookeeper在设定时间内未收到心跳包即判定为宕机，然后通知Master将宕机RegionServer上的Region重新分配到其他正常RegionServer机器中。 Zookeeper集群中保存了HBase的元数据信息，包括-Root-表存放的RegionServer位置，HBase集群表信息、Region信息等。 在Client对HBase集群进行读写时，Client并不与Master通信，而是通过Zookeeper获得-Root-表的存放地址，再由-Root-表获得对应的-Meta-表存放位置，最后通过-Meta-表获得读写Region的存放位置，从而完成读写过程。(Client——Root表——Meta表——数据最终存储地址，Client共需要跳转三次即可。-Root-表主要是为了防止数据量过大造成-Meta-表过大影响读写性能而设计的，但在HBase的0.96.x以后，由于考虑到正常情况下-Meta-表即可满足大多数数据需求，因此取消了-Root-表，使原来的读写过程从跳转三次，减少到调整两次，提供了HBase集群性能）3.3 MasterMaster主要是对RegionServer进行管理，包括将Region分配至RegionServer,HBase集群的负载均衡、在某个RegionServer宕机后，将该机器上的Region分配至正常的RegionServer，同时根据故障机器上的HLog，在正常RegionServer上完成数据恢复(该工作是恢复故障机器上MemStore中的数据)。由于Client除了新建/删除表时才会与Master通信，正常读写不需要与Master通信，因此Master并不会成为HBase集群性能的瓶颈，即使在Master故障的情况下，也不会影响Client对HBase集群的读写，只会影响新建/删除表等操作。3.4 RegionServerRegionServer是HBase集群中极其重要的部分，是数据的真正管理者，Client对HBase集群的读写均由RegionServer来完成，因此本文将RegionServer单独作为一小节来重点讲解。 4. RegionServer结构 4.1 HLog与MemStore每个RegionServer中均包含一个HLog、一个BolckCache和多个Region，每个Region包含多个Store，每个Store对应Table的ColumnFamily。每个Store中包含一个MemStore缓存和多个StoreFile（HFile）。当Client向RegionServer写入数据时，RegionServer首先将数据写入HLog，HLog将数据连同数据对应的Store信息顺序写入磁盘。数据写入HLog后，RegionServer会将数据再写入对应Store的MemStore中进行缓存，当MemStore缓存满(默认为64M大小)，MemStore会将数据溢出到磁盘形成临时文件，最后会将这些临时文件合并成为HFile文件。当数据从MemStore溢出到磁盘完成持久化后，HLog对应的数据则会被删除，从而保证HLog文件不会越来越大。之所以设计HLog机制，是为了在RegionServer宕机时，防止MemStore缓存内还未来得及溢出到磁盘的数据被丢失，由于HLog的存在，即使RegionServer宕机，也可通过HLog文件将MemStore缓存中的数据进行恢复。而MemStore则是通过对数据先缓存，待达到一定数据量后再集中溢出磁盘，从而减少了磁盘IO，提升了写入速度。用户可以通过设置将HLog机制关闭(针对可以容忍小部分数据丢失的场景)，这样可以提升HBase2~3倍的写入性能。同时由于MemStore作为内存缓存，属于JVM内存的一部分，MemStore的大小设置对HBase的写入性能也有一定的影响，需要根据不同的应用场景综合考虑。 4.2 BlockCache每个HRegionServer中存在一个BlockCache。相对于HLog和MemStore，BlockCache很少被提及，但确是很重要的一种缓存。HBase在读取数据时，首先会从MemStore中查询数据(最新的数据保存在此)，然后是从BlockCache中查询数据，当两者都为命中时，才会从HFile中读取数据。BlockCache用于缓存HFile中的热点数据，用于提升HBase的读取性能。与CPU中的一级缓存、二级缓存的原理类似，BlockCache将HFile中经常被读取的数据缓存至其中，当Client读取数据时，若数据被命中，则直接从BlockCache直接读取数据，减少从HFile读取的磁盘IO次数，从而提升HBase读取速度。BlockCache的大致工作原理如上所述，其中涉及很多细节以及许多参数需要进行调优。首先与MemStore不同的是，BlockCache由于需要满足随机读取，因此BlockCache缓存中会产生大量内存碎片，由于BlockCache采用JVM中的内存，当内存碎片达到一定数量时即需要进行碎片整理，这个过程会导致HBase的读取性能显著下降，甚至产生Full GC问题导致RegionServer卡死或宕机。因此BlockCache的内存回收算法和内存分配机制是一个关键的性能调优点。BlockCache默认采用JVM的内存，并使用LRU内存回收算法，因此回收内存时会产生性能问题，影响读取性能。因此，在HBase接下来的版本中，又引入了BucketBlockCache，并在该方式下设置了三种模式heap(JVM堆内存)、offheap(堆外内存)、file(SSD存储)。特别是BucketBlockCache的offheap模式，直接利用操作系统中的内存机制而非JVM堆内存，避免了内存回收产生的问题。应用中我们一般采用LRUBlockCache与BucketBlockCache(offheap模式)相结合的方式CombinedBlockCache，该方式下LRUBlockCache存储Index Block，BucketBlockCache存储Data Block(HFile的数据分为Index Block和Data Block等多种)。BlockCahce的内存大小影响HBase的读取性能，MemStore影响HBase的写入性能，同时HBase规定BlockCache与MemStore的内存大小之和不得大于JVM总内存的80%(为运算留足内存空间)，因此我们需要针对不同的应用场景(读多写少/读少写多)来合理安排BlockCache与MemStore在JVM内存中所占比例，从而达到性能最优。 4.3 HFileHBase中数据按照Table——Row——ColumnFamily——Qualifier——Value——TimeStamp(Version)，对HBase的Write/Update操作，都会更新KeyValue的版本(版本一般采用TimeStamp时间戳)。用户可以配置每种ColumnFamily最大版本数和数据最长存储时间，Client读取数据时可设定需要读取的版本数。HFile文件被切分为大小相同的Block，BlockSize可配置，默认64KB，BlockSize大有利于顺序Scan，BlockSize小有利于随机查询。 在HFile的众多Block中，DataBlock用来存储用户的KeyValue数据，Bloom相关的的Block主要用来完成BloomFilter。BloomFilter采用位图的方式，将Row/Row-ColumnFamily进行索引，当数据写入时将Rowd/Row-ColumnFamily的Hash值对应的位置1，当数据读取时首先去查找BloomFilter位图中相应位是否为1 ，为1则说明数据在该HFile中，去DataBlock读取数据，为0则说明数据不在该HFile中。通过BloomFilter可以减少磁盘IO次数，提升HBase读取性能。 ps: 参考文献及图片来源：有态度的HBase/Spark/BigData以及其他网络内容，未注名出处，望见谅","tags":[{"name":"HBase","slug":"HBase","permalink":"http://yoursite.com/tags/HBase/"}]},{"title":"ElasticSearch系列1——简介及基本架构","date":"2017-08-22T12:11:10.000Z","path":"2017/08/22/ElasticSearch系列1—简介及基本架构/","text":"1. 简介ElasticSearch是一个开源的分布式搜索和分析引擎，它在Lucence基础上做了二次开发，将Lucence繁琐的API调用变为简洁通用的ResetFul API，并提供了Java API，以及与Storm、Spark、Hadoop等大数据开源组件相适配的API接口。通过ElasticSearch，我们可以实现对JSON格式数据源的存储、文本搜索、结构化搜索和分析。ElasticSearch无论是单节点部署还是集群部署都较为简单，它可以自动完成数据存储、数据容灾、数据检索等多种功能，在维基百科、英国卫报、GitHub等得到了广泛应用。 2. Restful API REST直译为状态形式转移，是近几年极为火热的Client与Sever交互形式，之前的Client与Server主要采用RPC远程调用的方式，Server提供函数API，Client使用函数并将函数参数传递给Server，Server接收到函数执行相应动作并将结果返回。这种交互形式存在的主要弊端就是随着移动互联网的兴起，Client与Server需要同时提供对Web、IOS和Andriod的支持，这对开发和维护人员是一个很大的负担。而Rest形式的交互方式则统一了交互方式，多种Client可以使用统一的Restful API与Server通信。Client与Server的交互无非是对指定的数据进行一系列的操作，在Restful API中，我们将数据资源与执行动作分离开来，每个数据资源都可以通过URL获得，并且每个数据资源是无状态的，不依赖于其他资源，使用GET、PUT、POST、DELETE等指定动作，配合传入的一系列参数，可以实现所有交互，且交互简单，不存在这次交互依赖上次交互的问题(因为所有资源都是无状态的)。 3. 基础架构ElasticSearch架构中的关键概念分别为： 节点(Node)与主节点(Master Node) Index、Type、Dcoument与Field 主分片与复制分片 在介绍ElasticSearch架构之前，需要先介绍一下ES的存储逻辑。ES的存储逻辑分为索引(Index)、类型(Type)、文档(Documnet)、字段(Field)，对应关系型数据库，Index-&gt;Database，Type-&gt;Table，Dcoument-&gt;Row，Field-&gt;Column，数据源中的每个Json字符串，都会成为ES中的一个Document，Json中的字段成为Doucment中的Field，ES是基于文档来进行检索的。ES集群中存在多个Index，每一个Index又包含多个Type，每个Type包含多个Document，每个Document中又包含多个字段(Field)。ES不像其他一下大数据组件那样依赖HDFS，ES的存储由自己管理，并实现高可用(HA)。ES将每个Index的数据分成多个主分片(shard)，同时为了保证数据的高可用，每个主分片都有与之对应的复制分片。我们在新建Index时需要指定主分片的个数和每个主分片对应的复制分片个数，主分片数一旦确定即不可更改，复制分片可以在后期进行更改。 为了保证数据的高可用，分片均匀分布与各节点，主分片和复制分片不会存在于同一个Node，否则复制分片将失去意义。 在ElasticSearch集群中每个节点(Node)都是一个ElasticSearch实例，实例中的cluster.name决定该node属于哪个Cluster，我们将cluster.name设为同一个，组成一个ElasticSearch集群。在集群中存在Master Node，Master Node是通过选举产生的，主要工作为管理Index与Node，Client可以对集群中的所有节点(包括主节点)进行读写，因为每个节点都会根据Document ID查找到文档对应的分片(对Document Id的hash值按照主分片数进行求余来判定该文档属于该Index下的那个分片，因此Index的主分片数一经确定即不可更改，同时也说明Document ID用户在设定时需多方面考虑，也可让ES自动生成)，因此主节点的工作负载较轻，不会成为集群的性能瓶颈。上图中NODE1、NODE2、NODE3构成了一个ES集群，NODE1为主节点，我们在该集群上新建一个索引Index，该索引分为3个主分片P0、P1、P2，每个主分片有对应2个复制分片。从图中可以看出，主分片和复制分片没有在同一个Node上，实现了数据的高可用。ES将集群的健康状态分为了3种，我们可以通过命令：curl -XGET &#39;主机:9200/_cluster/health?pretty&#39;来获得。 green 所有主分片和复制分片都可用 yellow 所有主分片都可用，但不是所有复制分片都可用 red 不是所有主分片都可用4. 读写过程4.1 新建、索引与删除文档 新建、索引与删除文档都是属于对ES集群的Write操作，Write操作需要对Index的主分片进行操作，过程如上图所示： 如步骤1，客户端可以对集群中的任意一个节点发起新建、索引与删除文档的请求，比如Node1接收到Write请求，首先会根据请求文档的Document Id经过计算得出该文档属于哪个主分片，比如属于P0分片，Node1则将该请求转发至Node3； 如步骤2，Node3接收到Node1转发过来的Write请求，则在P0分片中进行Write操作，执行成功后再将请求转发至P0对应的复制分片R0上； 如步骤3，Node1、Node2节点存储有P0分片的复制分片R0，Node1、Node2在接收到Node3转发来的请求，会对复制分片R0进行操作，操作成功后将结果返回Node3； Node3在接收到所有复制分片都操作成功后，将Write的操作结果返回给Node1，由Node1将最终结果返回给客户端。 4.2 检索文档 检索文档属于对ES集群的Read操作，Read操作可以对Index的任意分片进行操作，包括主分片和复制分片，过程如上图所示： 如步骤1，客户端可以对集群中的任意一个节点发起检索文档的请求，比如Node1接收到Read请求，首先根据请求文档的Document Id经过计算得出该文档哪个分片，比如属于分片0，对于Read请求，为了平衡负载，请求节点会为每个请求选择不同的分片——它会循环所有分片副本。比如Node1决定读取Node2中的复制分片R0； 如步骤2，Node2接收到Node1转发的Read请求，对R0分片进行检索，并将检索结果返回至Node1； 如步骤3，Node1接手到Node2发来的检索结果，并将结果发送至客户端。 4.3 局部更新 局部更新即Update操作是Read操作与Write操作的结合，过程如上图所示： 如步骤1，客户端可以对集群中的任意一个节点发起Update请求，比如Node1接收到Update请求，首先根据请求文档的Document Id经过计算得出该文档哪个分片，比如属于分片0，Node1找到分片0的主分片P0在Node3，并将请求转发至Node3； 如步骤2和3，Node3接收到Node1转发的Update请求，对P0主分片进行检索，获得检索结果后对Document进行修改，并重新建立该文档的索引，操作成功后，将新建的该文档索引发送至复制分片R0； 如步骤4，复制分片R0在Node1和Node2对该文档重建索引，并将操作结果返回Node3； Node3收到所有复制分片均操作成功，则将结果发送至Node1，由Node1将结果发送至客户端； 4.4 批量请求批量请求其实是被请求节点将所有请求分发至其他节点，由其他节点进行处理，并将各节点的返回结果进行汇总，然后将汇总结果发送至客户端。 5. 插件介绍网上存在多种插件与ElasticSearch配合，实现更多功能。 官方提供的Kibana可以实现图表可视化等功能，可以在Kibana中实现ES的索引配置、Mapping配置、多种过滤条件检索，将检索结果保存并以多种酷炫的图表方式展示出来，显示效果惊艳，由于是官方插件，因此与ES的配合程度很高，但较为消耗机器资源。 官方提供的Security(旧称Shield)插件，提供了对ElasticSearch的权限管理，增添的用户权限管理，能够很好的保证数据安全，防止盗窃、篡改和监听。(但该插件是需要付费的，因此寻求免费用户权限管理，需要考虑在Nginx服务器上进行相应处理) 官方提供的Monitoring(旧成Marvel)插件，实现了对ElasticSearch集群的状态监控，我们可以直接监控集群健康状态、节点资源使用情况、索引状态等内容。(该插件也需要付费，但有免费授权) 除了官方提供的插件，GitHub上也有一些开源的ES插件，其中比较典型的为Head插件，该插件提供了集群管理、数据可视化等诸多功能，我们可以从GitHub上免费下载并安装，需要主要的是Head对ES2.x版本是以插件的形式运行，对ES5.x版本是以独立的服务运行的，安装过程不同。","tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://yoursite.com/tags/ElasticSearch/"}]},{"title":"新的征程","date":"2017-08-20T02:39:19.000Z","path":"2017/08/20/新的征程/","text":"最近公司安排了比较富裕的时间，让我进行自学，最近一年虽然做了一些大数据包括JStorm、Spark、ElasticSearch等项目，但仍然停留在使用阶段，对其中的具体细节并没有有一个深入的理解，也没有进行很好的梳理这才有了上半年的JStorm源码分析，但鉴于时间原因，对JStorm的源码的阅读，也仅仅停留在Nimbus、Supervisor启动、Topology的提交阶段。因此正好利用这个难得的学习时间段，系统的研究一下Spark、HBase、ElasticSearch、Kafka这几个大数据组件，正好包括了从数据输入端到计算引擎，再到数据库存储、数据检索整个大数据处理流程。通过近些天的学习，由于公司还没有安排具体的项目进行实践，因而计划在学习过程中，对所学内容在博客上进行一个系统的梳理。有时候静下心来想想，博客更大程度上是写给自己看的，只有不断的梳理知识点，不断的改进与完善，才能真正将知识点转变为自身的内在。近一段时间内，主要会写《Spark系列》、《HBase系列》、《ElasticSearch系列》等内容，更多的是对所学知识的一个梳理与总结。 ps:最近在学习的过程中，看了网上以下大牛写的技术博客，文章的严谨性、系统性、专业性以及配图，让我感触很深，在今后我也会好好写自己的博客，使之更专业，更严谨，而不仅仅是当作学习笔记或学习总结。这里有一些好的博客网址，推荐给大家： HBase： 有态度的HBase/Spark/BigDataSpark: 守护之鲨大数据网站：过往记忆大数据网站由于种种原因，换了公司，新的征程已经开始，我不是技术大神，但我在成为大神的路上，前行……((写给自己，明年的这个时候，不知又是怎样的情景，Fighting!!!))","tags":[{"name":"随笔","slug":"随笔","permalink":"http://yoursite.com/tags/随笔/"}]},{"title":"JStorm源码阅读系列4——SubmitTopology分析","date":"2017-03-30T12:56:10.000Z","path":"2017/03/30/JStorm源码阅读系列4—SubmitTopology分析/","text":"一、StormSubmitter.class submitTopology方法 在完成Spout类开发-&gt;Bolt类开发-&gt;通过setSpout和setBolt方法定义Topology结构并createTopology这几个步骤后，最后一步即是调用submitTopology方法，将我们定义的Topology作业提交到JStorm集群中运行。我们代码中调用的submitTopology方法并非在JStom源码中，而是JStorm提供的jstorm-core.jar包中backtype.storm.StormSubmitter.class类的方法。submitTopology方法用于向JStorm集群中提交Topology作业，Topology作业一旦提交成功则会一直运行下去，直到显示的kill掉。 1. submitTopology方法中首先会验证参数stormConf是否能进行json序列化，若不能则报异常。 2. 然后会调用Utils.readCommandLineOpts()和Utils.readStormConfig()方法，从storm.option、default.yaml和storm.yaml中读取Storm的相关配置，并将配置中%JSTORM_HOME%参数替换为实际真实路径，然后与我们传递的参数stormConf(包含用户在storm整体的个人配置)，共同构成Storm的conf，最后将存有所有相关配置的Map conf转化为Json字符串serConf。 3. 通过localNimbus对象来判定Topology是本地模式运行还是集群模式运行，若为本地模式则调用本地相关的submitTopology方法。若为集群模式，则首先根据conf，调用NimbusClient.getConfiguredClient(conf)，新建NimbusClient对象。 4. 得到NimbusClient对象后，首先调用topologyNameExists(client, conf, name)通过client与集群中的Nimbus通信，判断集群中是否已经存在该name的Topology作业，若存在则不能正常提交我们新的作业，因此报topology already exists异常。然后，调用submitJar(client, conf)向集群中提交我们的Topology jar包。 5. 最后调用client.getClient().submitTopology(name, path, serConf, topology)方法，通过Thrift与Nimbus通信，使用sendBase(&quot;submitTopology&quot;, args)发送给JStorm集群。其中参数args为submitTopology_args类对象，该对象包含了Topolgoy名字name、jar的upload路径uploadedJarLocation、jsonConf和StormTopology对象，参数”submitTopology”为methodName，集群通过Thrift收到请求，并根据methodName调用相应方法。 二、StormSubmitter.class submitJar方法 1. submitJar方法用来向JStorm的Nimbus提交我们的Toplogy jar包，首先得到我们的本地Topology jar包名localJar，然后使用NimbusClient与Nimbus通信,获得jar包的上传路径； 2. 若我们设置了TOPOLOGY_LIB_NAME和TOPOLOGY_LIB_PATH，则需要上传Topology依赖的lib，若没有则跳过。 3. 最后调用ubmitJar(conf, localJar, uploadLocation, client)方法，使用client.getClient().uploadChunk(uploadLocation, ByteBuffer.wrap(toSubmit))将jar包上传至Nimbus中。","tags":[{"name":"JStorm","slug":"JStorm","permalink":"http://yoursite.com/tags/JStorm/"}]},{"title":"JStorm源码阅读系列3——TopologyBuilder分析","date":"2017-03-26T02:34:35.000Z","path":"2017/03/26/JStorm源码阅读系列3—TopologyBuilder分析/","text":"一、TopologyBuilder.class Topology模版 TopologyBuilder提供了Java API的模版，我们通过TopologyBuilder可以建立属于自己的Topology并调用submitTopology方法即可在JStorm集群上启动该Topology。在JStorm源码中，TopologyBuilder.class中有如下注释，为我们提供了建立Topology的模版：1234567891011121314151617181920212223242526272829//集群模式下 TopologyBuilder builder = new TopologyBuilder(); builder.setSpout(&amp;quot;1&amp;quot;, new TestWordSpout(true), 5); builder.setSpout(&amp;quot;2&amp;quot;, new TestWordSpout(true), 3); builder.setBolt(&amp;quot;3&amp;quot;, new TestWordCounter(), 3).fieldsGrouping(&amp;quot;1&amp;quot;, new Fields(&amp;quot;word&amp;quot;)).fieldsGrouping(&amp;quot;2&amp;quot;, new Fields(&amp;quot;word&amp;quot;)); builder.setBolt(&amp;quot;4&amp;quot;, new TestGlobalCount()).globalGrouping(&amp;quot;1&amp;quot;); Map conf = new HashMap(); conf.put(Config.TOPOLOGY_WORKERS, 4); StormSubmitter.submitTopology(&amp;quot;mytopology&amp;quot;, conf, builder.createTopology());//本地模式下 TopologyBuilder builder = new TopologyBuilder(); builder.setSpout(&amp;quot;1&amp;quot;, new TestWordSpout(true), 5); builder.setSpout(&amp;quot;2&amp;quot;, new TestWordSpout(true), 3); builder.setBolt(&amp;quot;3&amp;quot;, new TestWordCounter(), 3).fieldsGrouping(&amp;quot;1&amp;quot;, new Fields(&amp;quot;word&amp;quot;)).fieldsGrouping(&amp;quot;2&amp;quot;, new Fields(&amp;quot;word&amp;quot;)); builder.setBolt(&amp;quot;4&amp;quot;, new TestGlobalCount()).globalGrouping(&amp;quot;1&amp;quot;); Map conf = new HashMap(); conf.put(Config.TOPOLOGY_WORKERS, 4); conf.put(Config.TOPOLOGY_DEBUG, true); LocalCluster cluster = new LocalCluster(); cluster.submitTopology(&amp;quot;mytopology&amp;quot;, conf, builder.createTopology()); Utils.sleep(10000); cluster.shutdown(); 在开发JStorm作业时，我们首先建立自己的Spout和Bolt类，处理相关的逻辑，然后使用TopologyBulider中的SetSpout与SetBolt方法确定各Spout与Bolt之间的数据流，最后调用TopologyBuilder的createTopology方法建立StormTopology,并将该Topology提交到JStorm集群中运行。 二、ComponentCommon.class 1. 在此之前，我们首先介绍ComponentCommon.class。CompoenentCommon对象包含了各个Spout和Bolt的配置信息，正是通过该对象，才能完美描绘出整个Topology的数据流拓扑结构，其中包括每个数据流所包含的Fields(字段)、各个Spout和Bolt之间的Grouping信息等。 2. ComponentCommon主要包含以下域：1234private Map&lt;GlobalStreamId,Grouping&gt; inputs; // requiredprivate Map&lt;String,StreamInfo&gt; streams; // requiredprivate int parallelism_hint; // optionalprivate String json_conf; // optional Map&lt;GlobalStreamId,Grouping&gt; inputs中的GlobalSteamId由ComponentId和StreamId共同构成，Grouping包含分组信息。Map&lt;String,StreamInfo&gt; streams中的String是指StreamId，StreamInfo包含了每个数据流中所包含了Fields(字段)，这些字段其实是由我们在各个Spout和Bolt中的declareOutpoutFields方法中进行定义的。parallelism_hint指并行度，即每个Spout或Bolt在集群中的Task总数。该字段为可选的，若没有设置该字段，则默认为1。json_conf为每个Spout和Bolt的Json字符串形式的配置信息。该配置信息可在每个Spout或Bolt中的getComponentConfiguration()方法中进行配置。 三、TopologyBuilder.class setSpout方法1234567891011121314151617181920public SpoutDeclarer setSpout(String id, IRichSpout spout, Number parallelism_hint) &#123; validateUnusedId(id); initCommon(id, spout, parallelism_hint); _spouts.put(id, spout); return new SpoutGetter(id); &#125; private void initCommon(String id, IComponent component, Number parallelism) &#123; ComponentCommon common = new ComponentCommon(); common.set_inputs(new HashMap&lt;GlobalStreamId, Grouping&gt;()); if (parallelism != null) &#123; common.set_parallelism_hint(parallelism.intValue()); &#125; else &#123; common.set_parallelism_hint(1); &#125; Map conf = component.getComponentConfiguration(); if (conf != null) common.set_json_conf(JSONValue.toJSONString(conf)); _commons.put(id, common); &#125; setSpout方法中，id为Spout的名字，IRichSpout是实现IRichSpout接口的对象实例，parallelism_hint是该Spout的并行度，即JStorm集群中运行该Spout的task总数。每一个task都是worker中的一个线程。 1. setSpout首先调用validateUnusedID(id)方法，检测_bolts、_spouts这两个map中是否已经包含该id，若包含则说明注册了已存在的id，则报重复异常。 2. 调用initCommon方法，建立该Spout对应的ComponentCommon对象，并对其初始化，ComponentCommon对象包含了数据流的分组方式Map&lt;GlobalStreamId，Grouping&gt; inputs，数据流所包含的output_fields信息Map&lt;String, StreamInfo&gt; stream， 并行度parallelism_hint，以及json格式的每个Component所对应的配置conf(该conf是独属于每个Componnet的，有别于Topology的conf)。配置完成后，将其放入全局变量Map&lt;String, ComponentCommon&gt; _commons中保存。 3. 调用_spouts.put(id，spout),将spout对象实例放入全局变量Map&lt;String, IRichSpout&gt; _spouts中。 四、TopologyBuilder.class setBolt方法123456789101112131415161718192021222324252627 public BoltDeclarer setBolt(String id, IRichBolt bolt, Number parallelism_hint) &#123; validateUnusedId(id); initCommon(id, bolt, parallelism_hint); _bolts.put(id, bolt); return new BoltGetter(id); &#125;``` setBolt方法与setSpout基本相同，将特定Bolt的相关配置放入全局变量_commons和_bolts中，唯一的不同在于最后的BoltGetter方法中，setSpout的SpoutGetter(id)并未做太多的工作，而BoltGetter(id)对象，包含了各个Grouping方法，并根据通过componentId和streamId构成GlobalStreamId，将每个GlobalSteamId所对应的Grouping方式存入ComponentCommon的inputs域，并最终将配置信息保存到全局变量_commons中。## 五、TopologyBuilder.class createTopology方法```java public StormTopology createTopology() &#123; Map&lt;String, Bolt&gt; boltSpecs = new HashMap&lt;String, Bolt&gt;(); Map&lt;String, SpoutSpec&gt; spoutSpecs = new HashMap&lt;String, SpoutSpec&gt;(); for (String boltId : _bolts.keySet()) &#123; IRichBolt bolt = _bolts.get(boltId); ComponentCommon common = getComponentCommon(boltId, bolt); boltSpecs.put(boltId, new Bolt(ComponentObject.serialized_java(Utils.javaSerialize(bolt)), common)); &#125; for (String spoutId : _spouts.keySet()) &#123; IRichSpout spout = _spouts.get(spoutId); ComponentCommon common = getComponentCommon(spoutId, spout); spoutSpecs.put(spoutId, new SpoutSpec(ComponentObject.serialized_java(Utils.javaSerialize(spout)), common)); &#125; return new StormTopology(spoutSpecs, boltSpecs, new HashMap&lt;String, StateSpoutSpec&gt;()); &#125; 1. 在createTopology()方法中，首先遍历_bolts和_spouts这两个map中所存储的boltId、spoutId，以及Bolt实例和Spout实例，然后调用getComponentCommon方法，该方法从_commons中获得setSpout和setBolt时所设置的ComponnetCommon对象，并调用Spout实例和Bolt实例中的declareOutputFields方法，初始化ComponentCommon对象的Map&lt;String, StreamInfo&gt; streams域，streams保存了每个数据流所包含的各个字段。然后将Bolt和Spout实例进行Java序列化生成ComponentObject对象(这里需要注意的是，因为creatTopology方法需要将我们自己定义的Bolt和Spout类对象进行序列化，所以我们在定义Bolt和Spout不要在其构造器中新建不能被序列化的对象，即使需要新建不能序列化的对象，那也应该放在prepare方法中进行。作者在一开始开发JStorm作业时就曽碰到过这种问题，就是因为在类的构造器中新建了不能被序列化的对象，修改为在prepare方法中新建该对象，即可解决此类异)，与之前得到的ComponentCommon对象一起存入Map&lt;String, Bolt&gt; boltSpecs和Map&lt;String, SpoutSpec&gt; spoutSpecs中。 2. 最后调用StormTopology的构造器，用每个Bolt和Spout的Java序列化对象和包含数据流各种信息的ComponentCommon对象，共同生成StormTopology对象。 12345678private ComponentCommon getComponentCommon(String id, IComponent component) &#123; ComponentCommon ret = new ComponentCommon(_commons.get(id)); OutputFieldsGetter getter = new OutputFieldsGetter(); component.declareOutputFields(getter); ret.set_streams(getter.getFieldsDeclaration()); return ret;&#125;","tags":[{"name":"JStorm","slug":"JStorm","permalink":"http://yoursite.com/tags/JStorm/"}]},{"title":"JStorm源码阅读系列1——Nimbus启动流程","date":"2017-03-22T15:16:30.000Z","path":"2017/03/22/JStorm源码阅读系列1—Nimbus启动流程/","text":"一、NimbusServer.class main方法 运行$JSTORM_HOME/bin/jstorm nimbus即可运行Nimbus启动程序，JStorm会调用com.alibaba.jstorm.daemon.nimbus包中的NimbusServer.class的main方法。 1. 调用Utils.readStormConfig()方法，从default.yaml、storm.yaml和storm.options中读取JStorm的相关配置，返回(Map)conf，后面的许多操作都要用到conf。 2. 新建DefaultInimbus实例，DefaultInimbus类实现了INimbus接口，实现了allSlotsAvailableForScheduling方法，在该方法中可以得到整个可以供Nimbus调度的Collection。1234567891011@Override public Collection&lt;WorkerSlot&gt; allSlotsAvailableForScheduling(Collection&lt;SupervisorDetails&gt; existingSupervisors, Topologies topologies, Set&lt;String&gt; topologiesMissingAssignments) &#123; // TODO Auto-generated method stub Collection&lt;WorkerSlot&gt; result = new HashSet&lt;WorkerSlot&gt;(); for (SupervisorDetails detail : existingSupervisors) &#123; for (Integer port : detail.getAllPorts()) result.add(new WorkerSlot(detail.getId(), port)); &#125; return result; &#125; 3. 新建NimbusServer实例instance，并调用instance.launchServer方法。 二、NimbusServer.class lauchServer方法 1. 调用StormConfig.validate_distributed_mode(conf)验证是否为distributed模式。 2. createPid(conf)在storm.local.dir/nimbus中创建nimbus对应pid的文件。 3. 调用initShutdownHook()方法—&gt;NimbusServer.cleanup()。 4. createNimbusData()方法，建立NimbusData对象，NimbusData类包含了Nimbus需要用到的各种数据，包括配置信息conf，与Zookeeper通信的StormZkClusterState，tasksHeartbeat，JStormMetricCache等。在NimbusData的构造器中，会完成对各种域的初始化工作。 5. 调用initFollowerThread方法，新建并启动FollowerRunnable后台服务线程(daemon线程)。 6. 调用ConfigExtension.getNimbusDeamonHttpserverPort(conf)方法，得到NimbusHttpserver的port,初始化Httpserver,并启动Httpserver。 7. 调用initContainerHBThread(conf)，如果JStorm运行在Apsara/Yarn集群上则初始化container，若没有则返回null。 8. 循环检测Nimbus是否为leader,若不是则休眠5s，直到Nimbus是leader为止。 9. 调用init(conf)方法。 三、FollowerRunnable.class 1. 该类的构造器中首先得到nimbus的网络地址和端口，并通过StormZkClusterState与Zookeeper通信，调用StormZkClusterState的update_nimbus_slave和update_nimbus_detail方法，向Zookeeper注册host和detail。然后调用tryToBeLeader方法，通过StormZkClusterState的try_to_be_leader方法使Nimbus成为leader。 2. run()方法是在后台周期性循环执行的。在该类的run()方法中，首先从NimbusData中得到StormZkClusterState，然后每隔sleepTime进行周期性休眠，线程唤醒后会循环调用StormZkClusterState的接口，监测Nimbus是否为leader,如果不是则尝试将Nimbus设置为leader，直到成功为止。接下来会调用check()方法，并向Zookeeper更新Nimbus的信息。 3. Zookeeper在整个JStorm集群中有着非常重要的作用。 四、NimbusServer.class的init方法 1. 调用NimbusUtils.cleanupCorruptTopologies(data)方法，清除在Zookeeper中存在，在本地目录却不存在的Topology(Nimbus刚启动时，要先清除掉Zookeeper中无用的Topology)。 2. 调用initTopologyAssign方法，初始化Topology Assign后台服务线程，该daemon线程通过使用DefaultTopologyScheduler，用来进行Topology发布。 3. 调用initTopologyStatus方法，更新Zookeeper中的Topology状态。 4. 调用initCleaner方法，启动一个线程，每隔600s，清理storm.local.dir/supervisor/inbox目录，该目录是uploading topology的目录。 5. 调用ServiceHandler方法，新建ServiceHandler，该类是一个thrift server callback，是所有命令的入口，包括submitTopology、killTopology等众多命令。 6. 调用initMonitor()方法，初始化监控线程。 7. 调用initThrift()方法，初始化Nimbus的ThriftServer，并启动它，从而完成整个的Nimbus启动流程。","tags":[{"name":"JStorm","slug":"JStorm","permalink":"http://yoursite.com/tags/JStorm/"}]},{"title":"《Java编程思想》读书笔记——4","date":"2017-03-20T15:04:30.000Z","path":"2017/03/20/Java编程思想读书笔记4/","text":"《Java编程思想》第18章——Java I/O系统 File类既能代表一个特定文件的名称，又能代表一个目录下的一组文件的名称。如果它指的是一个文件集，我们就可以对此集合调用list()方法，这个方法会返回一个字符数组。 通过继承，任何自InputStream或Reader派生而来的类都含有名为read()的基本方法，用于读取单个字节或者字节数组。同样，任何字OuputStream或Writer派生而来的类都含有名为write()的基本方法，用于写单个字节或自己数组。但是我们不会用到这些方法，它们之所以存在是因为别的类可以使用它们，以便提供更有用的接口。因此，我们很少使用单一的类来创建流对象，而是通过叠合多个对象来提供所期望的功能。 实际上，Java中“流”类库让人迷惑的主要原因就在于：创建单一的结果流，却需要创建多个对象。 System.setIn(InputStream)可以对标准输入重定向，System.setOut(PrintStream),System.setErr(PrintStream)是对标准输出和错误I/O流进行重定向。 要序列化一个对象，首先要创建某些OutputStream对象，然后将其封装在一个ObjectOutputStream对象内。这时，只需要调用writeObject()即可将对象序列化，并将其发送给OutputStream(对象序列化是基于字节的，因此要使用InputStream和OutputStream继承层次结构)。要反向进行该过程(即将一个序列还原为一个对象)，需要将一个InputStream封装在ObjectInputStream内，然后调用readObject()。我们最后获得的是一个引用，它指向一个向上转型的Object，所以必须向下转型才能直接设置它们。 《Java编程思想》第19章——枚举类型 如果你打算定义自己的方法，那么必须在enum实例序列的最后添加一个分号。同时，Java要求你必须先定义enum实例。如果在定义enum实例之前定义了任何方法或属性，那么在编译时就会得到错误信息。 虽然一般情况下我们必须使用enum类型来修饰一个enum实例，但是在case语句中却不必如此。 《Java编程思想》第20章——注解 @Override，表示当前的方法定义将覆盖超类中的方法。如果你不小心拼写错误，或者方法签名对不上被覆盖的方法，编译器就会发出错误提示。 @Deprecated，如果程序员使用了注解为它的元素，那么编译器会发出告警信息。 @SuppressWarnings，关闭不当的编译信息警告信息。在Java SE5之前的版本中，也可以使用该注解，不过会被忽略不起作用。 《Java编程思想》第21章——并发 事实上，从性能的角度看，如果没有任务会阻塞，那么在单处理器机器上使用并发就没有任何意义。 像Java所使用的这种并发系统会共享诸如内存和I/O这样的资源，因此编写多线程程序最基本的困难在于协调不同线程驱动的任务之间对这些资源的使用，以使得这些资源不会同时被多个任务访问。 Java的线程机制是抢占式的，这表示调度机制会周期性地中断线程，将上下文切换到另一个线程，从而为每个线程都提供时间片，使得每个线程都会分配到数量合理的时间片去驱动他的任务。 一个线程其实就是在进程中的一个单元的顺序控制流。 要定义一个任务，只需实现Runnable接口并编写run()方法，使得该任务可以执行你的命令。 任务的run()方法通常总会有某种形式的循环，使得任务一直运行下去直到不再需要，所以要设定跳出循环的条件。 Thread构造器只需要一个Runnable对象。调用Thread对象的start()方法为该线程执行必需的初始化操作，然后调用Runnable的run()方法，以便在这个新线程中启动该任务。 如果在你的机器上有多个处理器，线程调度器将会在这个处理器之间默默地分发线程。同时，线程调度机制是非确定的。 Executor允许你管理异步任务的运行，再无须显示地管理线程的生命周期。Executor在Java SE5/6中是启动任务的优选方法。 Runnable()是执行工作的独立任务，但是它不返回任何值。如果你希望任务在完成时能够返回一个值，那么可以实现Callable接口而不是Runnable接口。 因为异常不能跨线程传播回main()，所以你必须在本地处理所有在任务内部产生的异常。 调度器将倾向于让优先权最高的线程先执行。然而，这并不是意味着优先权较低的线程将得不到执行(也就是说，优先权不会导致死锁)。优先级较低的线程仅仅是执行的频率较低。 你可以在一个任务的内部，通过调用Thread.currentThread()来获得对驱动该任务的Thread对象的引用。通过Thread.currentThread().setPriority()来修改当前线程的优先级。 注意，优先级是在run()的开头部分设定的，在构造器中设置它们不会有任何好处，因为Executor在此刻还没有开始执行任务。 所谓后台(daemon)线程，是指在程序运行的时候在后台提供一种通用服务的线程，并且这种线程并不属于程序中不可或缺的部分。 必须在线程启动(调用start())之前调用setDaemon()方法，才能把它设置为后台线程。 如果是一个后台线程，那么它创建的任何线程将被自动设置成后台线程。 线程组持有一个线程的集合。但是，最好把线程组看成一次不成功的尝试，你只要忽略它就好了。 Java以提供关键字synchronized的形式，为防止资源冲突提供了内置支持。当任务要执行synchronized关键字保护的代码片段时，它将检查锁是否可用，然后获取锁，执行代码，释放锁。 所有对象都自动含有单一的锁(也称为监视器)。当在对象上调用其任意synchronized方法的时候，此对象都被加锁，这时该对象上的其他synchronized方法只有等到前一个方法调用完毕并释放了锁之后才能被调用。 注意，在使用并发时，将域设置为private是非常重要的，否则，synchronized关键字就不能防止其他任务直接访问域，这样就会产生冲突。 每个访问临界共享资源的方法都必须被同步，否则它们就不会正确的工作。 大体上，当你使用synchronized关键字时，需要写的代码量更少，并且用户错误出现的可能性也会降低，因此通常只有在解决特殊问题时，才使用显示的Lock对象。 理解原子性和易变性是不同的概念这一点很重要。在非volatile域上的原子操作不必刷新到主内存中去，因此其他读取该域的任务也不必看到这个新值。如果多个任务在同事访问某个域，那么这个域就应该是volatile的，否则，这个域就应该只能经由同步来访问。同步也会导致向主内存中刷新，因此如果一个域完全由synchronized方法或语句块来访问，那就不必将其设置为是volatile的。 使用volatile而不是synchronized的唯一安全的情况是类中只有一个可变的域。再次提醒，你的第一个选择应该是使用synchronized关键字，这是最安全的方式，而尝试其他任何方式都是有风险的。 有时，你只是希望防止多个线程同时访问方法内部的部分代码，而不是防止访问整个方法。通过这种方式分离出来的代码段被称为临界区，它也使用synchronized关键字建立，这里，synchronized被用来指定某个对象，此对象的锁被用来对花括号内的代码进行同步控制。这也称为同步控制块。 synchronized块必须给定一个在其上进行同步的对象，并且最合理的方式是，使用其方法正在被调用的当前对象：synchronized(this)。 当你调用wait()方法时，就是在声明：“我已经刚刚做完能做的所有事情，因此我要在这里等待，但是我希望其他的synchronized操作在条件适合的情况下能够执行”。 实际上，只能在同步控制方法或同步控制块里调用wait()、notify()和notifyAll()。 事实上，当notifyAll()因某个特定锁而被调用时，只有等待这个锁的任务才会被唤醒。 LinkedBlockingQueue和ArrayBlockingQueue可用于同步。 Java对死锁并没有提供语言层面上的支持；能否通过仔细地设计程序来避免死锁，这取决于你自己。 ConcurrentHashMap和ConcurrentLinkedQueue允许并发的读取和写入。 从JDK1.5开始Java并发包里提供了两个使用CopyOnWrite机制实现的并发容器,它们是CopyOnWriteArrayList和CopyOnWriteArraySet。CopyOnWrite容器非常有用，可以在非常多的并发场景中使用到。CopyOnWrite容器即写时复制的容器。通俗的理解是当我们往一个容器添加元素的时候，不直接往当前容器添加，而是先将当前容器进行Copy，复制出一个新的容器，然后新的容器里添加元素，添加完元素之后，再将原容器的引用指向新的容器。这样做的好处是我们可以对CopyOnWrite容器进行并发的读，而不需要加锁，因为当前容器不会添加任何元素。所以CopyOnWrite容器也是一种读写分离的思想，读和写不同的容器。(来源——网络博客)","tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"}]},{"title":"《Java编程思想》读书笔记——3","date":"2017-03-19T10:26:45.000Z","path":"2017/03/19/Java编程思想读书笔记3/","text":"《Java编程思想》第10章——内部类 在拥有外部类对象之前是不可能创建内部类对象的。这是因为内部类对象会暗暗地连接到创建它的外部类对象上。 接口的所有成员自动被设置为public。 匿名类不可能有构造器。 《Java编程思想》第11章——持有对象 如果一个类没有显式地声明继承自哪个类，那么它自动地继承自Object。 通过使用泛型，就可以在编译期防止将错误类型的对象放置到容器中。 你可以将Apple的子类型添加到被指定为保存Apple对象的容器中。 你可以直接使用Arrays.asList()的输出，将其当作List，但是在这种情况下，其底层表示的是数组，因此不能调整尺寸。 基本的ArrayList，长于随机访问元素，但是在List的中间插入和移除元素时较慢。LinkedList在List中间进行插入和删除的操作代价较低，并提供了优化的顺序访问，但在随机访问方面相对比较慢。 subList()所产生的列表的幕后就是初始化列表，因此，对所返回的列表进行修改，会直接反映到原初始列表中，反之亦然。 不能在foreach中直接删除容器的元素，但可以通过Iterator来实现。 Iterator的真正威力：能够将遍历序列的操作与序列底层的结构分离。因此，我们有时会说：迭代器统一了对容器的访问方式。 我们通常都会选择一个HashSet的实现，它专门对快速查找进行了优化。 队列常被当作一种可靠的将对象从程序的某个区域传输到另一个区域的途径。 能够与foreach一起工作是所有Collection对象的特性。如果你创建了任何实现Iterable的类，都可以将它用于foreach语句中。 不存在任何从数组到Iterable的自动转换，你必须手动执行这种转换。 像数组一样，List也建立数字索引与对象的关联，因此数组和List都是排好序的容器，List能够自动扩充容量。 《Java编程思想》第12章——通过异常处理错误 当覆盖方法的时候，只能抛出在基类方法的异常说明里列出的那些异常。 《Java编程思想》第13章——字符串 String对象是不可变。 当你为一个类编写toString()方法时，如果你要在toString()方法中使用循环，那么最好自己创建一个StringBuilder对象，用它来构造最终的结果。 一般来说，比起功能有限的String类，我们更愿意构造功能强大的正则表达式对象，只需要导入java.util.regex包，然后用static Pattern.compile()方法来编译你的正则表达式即可。它会根据你的String类型的正则表达式生成一个Pattern对象。接下来，把你想要检索的字符串传入Pattern对象的matcher()方法。matcher()方法会生成一个Matcher对象，它有很多功能可用，包括find等。 通过reset()方法，可以将现有的Matcher对象应用于一个新的字符序列。 在Java引入正则表达式和Scanner类之前，分割字符串的唯一方法是使用StringTokenizer来分词。现在基本上StringTokenizer已经可以废弃不用了。 《Java编程思想》第14章——类型信息 如果某个对象出现在字符串表达式中，toString()方法就会被自动调用，以生成表达该对象的String。 我们希望大部分代码尽可能少地了解对象的具体类型，而是与对象家族中的一个通用表示打交道。这样代码会更容易写，更容易读，且更便于维护，设计也更容易实现、理解和改变。所以“多态”是面向对象编程的基本目标。 要理解RTTI(Run-Time Type Information)在Java中的工作原理，首先必须知道类型信息在运行时是如何表示的，这项工作是由称为Class对象的特殊对象完成的，它包含了与类有关的信息。 类是程序的一部分，每个类都有一个Class对象。 所有的类都是在对其第一次使用时，动态加载到JVM中的。 一旦某个类的Class对象被载入内存，它就被用来创建这个类的所有对象。 如果你已经拥有了一个感兴趣的类型的对象，那就可以通过调用getClass()方法来获取Class引用了。 进行向下转型前，如果没有其他信息可以告诉你这个对象是什么类型，那么使用instanceof是非常重要的，否则会得到一个ClassCastException异常。 对instanceof有比较严格的限制：只可将其与命名类型进行比较，而不能与Class对象做比较。 instanceof保持了类型的概念，它指的是：“你是这个类吗？或者你是这个类的派生类吗？”，而如果用==比较实际的Class对象，就没有考虑继承——它或者是这个确切的类型，或者不是。 RTTI和反射之间真正的区别在于，对RTTI来说，编译器在编译时打开和检查.class文件，而对于反射机制来说，.class文件在编译时是不可获取的，所以在运行时打开和检查.class文件。因此使用反射机制时，那个类的.class文件对于JVM来说必须是可获取的；要么在本地机器上，要么可以通过网络取得。 面向对象编程语言的目的是让我们在凡是可以使用的地方都使用多态机制，只在必需的时候使用RTTI。 不要太早地关注程序的效率问题，这是个诱人的陷阱。最好首先让程序运作起来，然后再考虑它速度。 《Java编程思想》第15章——泛型 与其使用Object，我们更喜欢暂时不指定类型，而是稍后再决定具体使用什么类型，要达到这个目的，需要使用类型参数，用尖括号括住，放在类名后面。然后在使用这个类的时候，再用实际的类型替换此类型参数。 Java泛型的核心概念：告诉编译器想使用什么类型，然后编译器帮你处理一切细节。 是否拥有泛型方法，与其所在的类是否是泛型没有关系。 无论何时，只要你能做到，你就应该尽量使用泛型方法。也就是说，如果使用泛型方法可以取代将整个类泛型化，那么就应该使用泛型方法，因为它可以使事情更清楚明白。 《Java编程思想》第16章——数组 Java标准类库Arrays有一个作用十分有限的fill()方法：只能用同一个值填充各个位置，而针对对象而言，就是复制同一个引用进行填充。 Java标准类库提供有static方法System.arraycopy()，用它复制数组比用for循环复制要快很多。 Java标准类库中的排序算法针对正排序的特殊类型进行了优化——针对基本类型设计的“快速排序”，以及针对对象设计的“稳定归并排序”。 Java对尺寸固定的低级数组提供了适度的支持，这种数组强调的是性能而不是灵活性。 当你使用最近的Java版本编程时，应该“优先容器而不是数组”。只有在证明性能成为问题(并且切换到数组对性能提高有所帮助)时，你才应该将程序重构为使用数组。 《Java编程思想》第17章——容器深入研究 对于良好的编程风格而言，你应该在覆盖equals()方法时，总是同时覆盖hashCode()方法。 散列码是“相对唯一”的、用以代表对象的int值，它是通过将该对象的某些信息进行转换而生成的。hashCode()是根类Object中的方法，因此所有Java对象都能产生散列码。HashMap就是使用对象的hashCode()进行快速查询的，此方法能够显著提高性能。 默认的Object.equals()只是比较对象的地址。如果要使用自己的类作为HashMap的键，必须同时重载hashCode()和equals()。 hashCode()并不需要总是能够返回唯一的标识码。 设计hashCode()时最重要的因素就是：无论何时，对同一个对象调用hashCode()都应该生成同样的值。 如果你的hashCode()方法依赖于对象中易变的数据，用户就要当心了，因为此数据发生变化时，hashCode()就会生成一个不同的散列码，相当于产生了一个不同的键。 散列码不必是独一无二的(应该更关注生成速度，而不是唯一性)，但是通过hashCode()和equals()，必须能够完全确定对象的身份。","tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"}]},{"title":"JStorm源码阅读系列2——Supervisor启动流程","date":"2017-03-02T14:34:35.000Z","path":"2017/03/02/JStorm源码阅读系列2—Supervisor启动流程/","text":"一、Supervisor.class main方法 JStorm源码版本：2.1.0 在Linux中输入$JSTORM_HOME/bin/jstorm supervisor命令，即可运行supervisor启动程序，系统会调用com.alibaba.jstorm.daemon.supervisor包中Supervisor.class的main方法。在main方法中，首先构造Supervisor实例，并调用该实例的run方法。12345public static void main(String[] args) &#123; JStormServerUtils.startTaobaoJvmMonitor(); Supervisor instance = new Supervisor(); instance.run();&#125; 二、Supervisor.class run方法 1. 调用Utils.readStormConfig()方法，从default.yaml、storm.yaml和storm.options中读取JStorm的相关配置，返回(Map)conf，后面的许多操作都要用到conf。 2. StormConfig.distributed_mode(conf)验证JStorm模式是否为分布式模式，否则报出错异常。本文主要围绕distributed模式，暂不讨论local模式。 3. createPid(conf)在storm.local.dir/supervisor中创建supervisor对应pid的文件。 4. 调用mkSupervisor(conf,null)方法，创建并启动一个supervisor。 5. initShutdownHook(supervisorManager)，注册一个JVM ShutDown Hook方法。 6. 循环检测supervisorManager.shutdown方法是否被调用，若没有则阻塞1s并继续循环，否则打印“Shutdown supervisor”。 三、Supervisor.class mkSupervisor方法 1. 删除storm.local.dir/supervisor/tmp目录下的文件。 2. 创建与Zookeeper通信/操作的实例StormZkClusterState，在JStorm中，集群与Zookeeper通信都是通过StormZkClusterState类来实现的，Supervisor需要从Zookeeper获得分配到该节点的topology代码以及对应的tasks,同时把节点上的worker状态发送给Zookeeper，所以需要一个与 Zookeeper通信的实例。 3. 创建数据库LocalState实例，LocalState以Key-Value形式将相应信息读写到本地磁盘保存，Supervisor会把supervisorID等配置信息以及节点中运行的task也会把自身状态周期性的写入LocalState，LocalState会把这些信息放入本地磁盘保存。 4. 创建一个异步循环线程集合Vector threads。 5. 创建一个Heartbeat线程，并将该线程放入threads中，每隔几秒，调用Heartbeat的update方法，将SupervisorInfo信息，同步至Zookeeper。SupervisorInfo包括supervisor的hostname，supervisorid， 同步时间，与上次同步的间隔时间，workerPorts的Set，以及availableWorkerPorts的Set。通过将SupervisorInfo信息发送至Zookeeper,Nimbus可以实时监控supervisor状态以及worker的数量。 6. 创建管理worker的SyncProcessEvent线程，在该线程中，Supervisor实现启动新worker,关闭无用worker的功能。 7. 创建SyncSupervisorEvent线程，该线程每隔几秒运行一次，从Zookeeper读取Supervisor的相关信息，包括topolgoy的代码、tasks等，并调用SyncProcessEvent的run方法，来启动worker。 8. 启动Httpserver。 四、Assignment类 Assignment是Supervisor从Zookeeper得到的Topology的相关信息，包括Topology对应Supervisors的主机名、workers以及task的启动时间等信息。 五、LocalAssignment类 LocalAssignment是Supervisor在本地存储的Topology信息，包括topologyId、topologyName、taskIds(set)、mem、cpu、jvm等。LocalAssignment是Supervisor从Zookeeper读取Assignment并转换得到的。Supervisor周期性的从Zookeeper读取Assignment,得到Nimbus分配到该节点的Topology信息，并与节点本地存储的LocalAssignment对比，若有不同则对worker进行管理调度，启动新的worker，关闭无用的worker，使节点与Zookeeper中存储的信息相匹配。 六、SyncSupervisorEvent.class run方法 该方法是Supervisor中非常重要的方法，完成整个Topology的同步工作，根据Nimbus的调度，实现对节点中Topology的管理： 1. 从Zookeeper中读取topologyid对应的信息Map&lt;String, Assignment&gt; assignments。 2. 从storm.local.dir/supervisor/stormdist/本地目录中读取topologyid列表。 3. 从Zookeeper中获取zkAssignment，从localstate中获取localAssignment。 4. 对比zkAssignment和localAssignment，获取需要更新和重新下载的topologyid。 5. 从Zookeeper中获取对应topology在Nimbus存放jar包的目录，并调用Nimbus Client,从Nimbus的storm-dir/storm-dist/topologyid/storm.jar中下载代码，将jar包下载到本地目录。该部分通过层层方法调用，最终使用downloadFromMaster方法来实现的：12345678910111213141516171819202122public static void downloadFromMaster(Map conf, String file, String localFile) throws IOException, TException &#123; WritableByteChannel out = null; NimbusClient client = null; try &#123; client = NimbusClient.getConfiguredClient(conf, 10 * 1000); String id = client.getClient().beginFileDownload(file); out = Channels.newChannel(new FileOutputStream(localFile)); while (true) &#123; ByteBuffer chunk = client.getClient().downloadChunk(id); int written = out.write(chunk); if (written == 0) &#123; client.getClient().finishFileDownload(id); break; &#125; &#125; &#125; finally &#123; if (out != null) out.close(); if (client != null) client.close(); &#125;&#125; 其中的beginFileDownload、downloadChunk方法使用Thrift与Nimbus通信。 6. 运行syncProcessesEvent。 7. 触发heartbeat更新，启动心跳包发送线程，将supervisorInfo发送中Zookeeper。 七、SyncProcessEvent.class run方法 该方法在SyncSupervisorEvent被调用，通过该方法，Supervisor实现对节点中worker的管理，启动新的worker，关闭无用的worker: 1. 从LocalAssignment得到已经发布的tasks。 2. 根据localstate和localassignment得到每个worker的state，即节点中所有worker的状态，worker总数量是通过storm.yaml文件进行配置的，与ports对应。 3. 根据每个work的heartbeat来判断，关闭无用的workers。 4. checkNewWorkers(conf);检查新的workers。 5. checkNeedUpdateTopologys(localWorkerStats, localAssignments);检查需要更新的topology 6. 在startNewWorkers方法根据cluster_mode分为local和distributed，调用不同的lauchWorkers方法，启动新的workers。 八、SyncProcessEvent.class lauchWorker(distributed)方法 在SyncSupervisorEvent已经将每个topology的jar包从Zookeeper下载到了本地storm.local.dir/supervisor/stormdist/topologyId目录下，因此通过StringBuilder构造一个命令行，其中包括jstrom.log.dir、jstorm.home、java.library.path等配置以及com.alibaba.jstorm.daemon.worker.Worker、topologyId、portId等信息，然后调用JStormUtils.launch_process(cmd, environment, true)方法，即可调用com.alibaba.jstorm.daemon.worker.Worker类中的main方法。 九、Worker.class execute方法 在main方法中会创建Worker实例，并调用execute方法: 1. execute方法从Assignement得到topologyid-supervisorid-port对应的tasks，并create Thread(task)。 Worker每隔几秒将自身状态放入localstate KV database中。Supervisor调度worker时可以从localstate中得到worker状态。 2. worker是通过WorkerData来得到需要执行的tasks，而WorkerData中包含topologyId、supervisorId、port、worker、taskids(set)等信息。 十、总结 回顾整个Supervisor启动流程，Supervisor的作用可以总结为以下几点： 1. 周期性调用SyncSupervisorEvent，从Zookeeper得到相应的Topolgoy的信息Assignment，并将topology信息存入本地，同时将topology的jar包下载到本地，使节点运行的topology与Nimbus分配的topology保持同步。 2. 周期性调用SyncProcessEvent，管理Worker； 3. 周期性向Zookeeper发现心跳包，将SupervisorInfo发送至Zookeeper。 4. Supervisor中的Worker通过Woker.class类实现对task的管理，同事Worker将自身运行状态周期性的写入LocalState中，供Supervisor管理Worker使用。","tags":[{"name":"JStorm","slug":"JStorm","permalink":"http://yoursite.com/tags/JStorm/"}]},{"title":"JStorm源码阅读系列","date":"2017-03-02T13:43:35.000Z","path":"2017/03/02/JStorm源码阅读系列/","text":"一、JStorm简述 JStorm在使用自带的集群管理中，主要包括Nimbus、Zookeeper、Supervisor、Worker四部分，其中Nimbus作为调度器，其功能主要为调度管理topology，通过Zookeeper这一协调器，实时监测Supervisor发送过来的心跳包，当Supervisor出错时，Nimbus可以将故障节点上运行的task重新分配到其他正常节点上，实现JStorm集群较高的容错性。Supervisor管理节点上的workers，当某个worker出错时，Supervisor可以将该worker上运行的tasks，调度到其他worker上运行。Worker是Supervisor上运行的进程，一个Worker上面可运行多个task，每个Worker与节点的Port相对应。Zookeeper上存储了每个topology的信息、代码以及JStorm的的一些配置信息。Nimbus通过Zookeeper获得每个worker的心跳包，以及每个节点的信息，Supervisor通过Zookeeper获得每个Topology的信息，以及需要节点运行的task信息和代码。 由此可见，通过Zookeeper这一中间组件，Nimbus和Supervisor实现了信息共享，同时也避免了单点故障的出现，Supervisor挂掉，Nimbus可以将该节点上的tasks调度到其他节点上运行，即使Nimbus出现问题，由于Supervisor仍然可以从Zookeeper上得到相关信息，因此JStorm依然可以正常运行。实践证明，JStorm集群中运行的Topology有着极高的容错性和稳定性，即使出现节点宕机故障，可自动实现调度，将故障节点的task自动调度到其他正常节点。同时JStorm在实时分析中，实时性高，数据吞吐量大，集群部署简单，Topology开发简单快速，是一个值得推荐和使用的实时分析/流式计算引擎。 二、JStorm源码阅读系列 在进行一段JStorm的实时分析程序开发后，对于在JStorm的编程架构上进行程序开发已经较为熟练，但对于JStorm内部的运行机制，对应Nimbus、Zookeeper和Supervisor在JStorm所扮演的角色仍然没有较为深刻的理解，所以也就产生了阅读JStorm源码的年头，通过深入阅读源码，我相信一定能够加深对JStorm的理解，同时也为今后进行实时计算平台的开发打下基础。 《JStorm源码阅读系列1——Nimbus启动流程》《JStorm源码阅读系列2——Supervisor启动流程》《JStorm源码阅读系列3——TopologyBuilder分析》《JStorm源码阅读系列4——SubmitTopology分析》","tags":[{"name":"JStorm","slug":"JStorm","permalink":"http://yoursite.com/tags/JStorm/"}]},{"title":"《Java编程思想》读书笔记——2","date":"2017-02-21T13:21:21.000Z","path":"2017/02/21/Java编程思想读书笔记2/","text":"《Java编程思想》第5章——初始化与清理 构造器是一种特殊类型的方法，因为它没有返回值，这与返回值为void明显不同。 每个重载的方法都必须有一个独一无二的参数类型列表，甚至参数顺序的不同也足以区分两个方法，不过一般情况不这么做，因为这会使代码难以维护。需要注意的是，通过返回值来区分重载方法是行不通的。 如果你写的类中没有构造器，则编译器会自动帮你创建一个默认构造器。如果你已经定义了一个构造器（无论是否有参数），编译器就不会帮你自动创建默认构造器。 this关键字只能在方法内部使用，表示对“调用方法的那个对象”的引用。this关键字对于将当前对象传递给其他方法也很有用。 在构造器中可以通过this调用另一个构造器，但却不能调用两个，此外，必须将构造器调用置于最起始处，否则编译器会报错。在类中除构造器之外，编译器禁止在其他任何方法中调用构造器。 static方法就是没有this的方法，在static方法的内部不能调用非静态方法。 一旦垃圾回收器准备好释放对象占用的存储空间，将首先调用finalize()方法，并且在下一次垃圾回收动作发生时，才会真正回收对象占用的内存。所以可以用finalize()在垃圾回收时刻做一些重要的清理工作。但finalize()不能滥用，因为垃圾回收器和与垃圾回收有关的任何行为（包括finalize()），它们都必须同内存技巧回收有关，这就对finalize()的需求限制到一种特殊情况，即通过某种创建对象方式以外的方式为对象分配了存储空间。 如果JVM并未面临内存耗尽的情形，它是不会浪费时间去执行垃圾回收以恢复内存的。 在类的内部，变量定义的先后顺序决定了初始化的顺序。即使变量定义散布于方法定义之间，它们仍旧会在任何方法（包括构造器）被调用之前得到初始化。 static不能用于局部变量，它只能作用于域。 静态块，仅执行一次：当首次生成这个类的一个对象时，或者首次方法属于那个类的静态数据成员时。即静态初始化只在Class对象(区别于一般对象)首次加载的时候进行一次。 初始化顺序：父类的（静态变量、静态初始化块）=&gt; 子类的（静态变量、静态初始化块）=&gt; 父类的（变量、初始化块、构造器）=&gt; 子类的（变量、初始化块、构造器）。 《Java编程思想》第6章——访问权限控制 每一个class文件都只能有一个public类，public类的名称必须完全与含有该编译单元的文件名相匹配。 类既不可以是private也不可以是protected的(内部类除外)，类的访问权限仅有两个选择：包访问权限或public。 默认访问权限没有加任何关键字，通常是指包访问权限。 protected也提供包访问权限，也就是说相同包内的其他类可以访问protected类。 访问权限控制可以确保不会有任何客户端程序员依赖于某个类的底层实现的任何部分。 《Java编程思想》第7章——复用类 当创建一个类时，总是在继承，因此，除非已明确指出要从其他类中继承，否则就是在隐式地从Java的标准根类Object进行继承。 当创建了一个导出类的对象时，该对象包含了一个基类的子对象。这个子对象与你用基类直接创建的对象是一样的。Java会自动在导出类的构造器中插入对基类构造器的调用。 如果没有默认的基类构造器，或者想调用一个带参数的基类构造器，就必须用关键字super显示地编写调用基类构造器的语句，并配以适当的参数列表。同时，调用基类构造器必须是你在导出类构造器中要做的第一件事。 “is-a”的关系是用继承来表达的，而“has-a”的关系则是用组合来表达的。 “为新的类提供方法”并不是继承技术中最重要的方面，其最重要的方面是用来表现新类和基类之间的关系。 由于向上转型是从一个较专用类型向较通用类型转换，所以总是很安全的。 到底是该用组合还是继承，一个最清晰的判断方法就是问一问自己是否需要从新类向基类进行向上转型。如果必须向上转型，则继承是必要的；但如果不需要，则应当好好考虑自己是否需要继承。 对于基本类型，final使数值恒定不变，而用于对象引用，final使引用恒定不变。一旦引用被初始化指向一个对象，就无法再把它改为指向另一个对象，然而对象自身却是可以被修改的。 数组也是对象。 根据惯例，既是static又是final的域将用大写表示，并用下划线分隔各个单词。必须在域的定义处或者每个构造器中用表达式对final进行赋值，这正是final域在使用前总是被初始化的原因所在。 final确保在继承中使方法行为保持不变并且不会被覆盖。 类中所有private方法都隐式地指定为是final的。 当将某个类的整体定义为final时，就表明了你不打算继承该类，而且也不允许别人这样做。由于final类禁止继承，所以final类中所有的方法都隐式指定为是final的。 如果将项目视作是一种有机的、进化着的生命体而去培养，而不是打算像盖摩天大楼一样快速见效，就会获得更多的成功和更迅速的回馈。 《Java编程思想》第8章——多态 在面向对象的程序设计语言中，多态是继数据抽象和继承之后的第三种基本特征。多态也称作动态绑定、运行时绑定、后期绑定。 继承允许将对象视为它自己本身的类型或其他基类来加以处理。 将一个方法调用同一个方法的主体关联起来被称作绑定，若在程序执行之前进行绑定，叫做前期绑定。而后期绑定，它的含义就是在运行时根据对象的类型进行绑定。 Java中除了static方法和final方法之外，其他所有方法都是后期绑定。 那些操作基类接口的方法不需要任何改动就可以应用于新类。 多态是一项让程序员将改变的事物与未变的事物分离开来的重要技术。 任何域访问操作都将由编译器解析，因此不是多态的。 协变返回类型：在Java SE5中添加了协变返回类型，它表示在导出类中的被覆盖方法可以返回基类方法的返回类型的某种导出类型。 对于向下转型，如果所转类型是正确的，那么转型成功；否则，就会返回一个ClassCastException异常。 为了在自己的程序中有效地运用多态乃至面向对象的技术，必须扩展自己的编程视野，使其不仅包括个别类的成员和消息，而且还要包括类与类之间的共同特征以及它们之间的关系。 《Java编程思想》第9章——接口 Java提供了一个叫做抽象方法的机制，这种方法是不完整的；仅有声明而没有方法体。包含抽象方法的类叫做抽象类。 如果从一个抽象类继承，并想创建该新类的对象，那么就必须为基类中的所有抽象方法提供方法定义。如果不这样做，那么导出类便也是抽象类，且编译器将会强制我们用abstract关键字来限定这个类。 使某个类成为抽象类，并不需要所有的方法都是抽象的，所以仅需将某些方法声明为抽象的即可。 一个接口表示：“所有实现了该特定接口的类看起来都像这样”。 接口也可以包含域，但是这些域隐式地是static和final的。 可以继承任意多个接口，并可以向上转型为每个接口。 使用接口的核心原因：为了能够向上转型为多个基类类型（以及由此而带来的灵活性）。 可以通过继承，在新接口中组合数个接口。 接口中的域不是接口的一部分，它们的值被存储在该接口的静态存储区域内。","tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"}]},{"title":"serialVersionUID的用法","date":"2017-02-14T12:20:07.000Z","path":"2017/02/14/serialVersionUID用法/","text":"在Java编程中为了进行网络传输或方便磁盘存储，我们经常需要对Java对象进行序列化，Java的对象序列化就是将那些实现了Serializable接口的对象转换成字符序列，并能够在需要时反序列化为原来的Java对象。因此，只要对象实现了Serializable接口(该接口不包含任何方法)，即可对其进行序列化。 Java内置序列化与反序列化12345678910111213141516import java.io.Serializable;public class TestObject implements Serializable &#123; private int a; private long b; public TestObject(int a, long b)&#123; this.a = a; this.b = b; &#125; @Override public String toString() &#123; return \"a: \" + a + \" b: \"+ b; &#125;&#125; TestObject.class是我们定义的测试类TestObject，该类实现了Serializable接口，类中包含了字段a和b。import java.io.FileOutputStream;123456789101112131415import java.io.IOException;import java.io.ObjectOutputStream;public class TestWrite &#123; public static void main(String[] args) &#123; TestObject serialize = new TestObject(7, 18); try &#123; FileOutputStream fileOutputStream = new FileOutputStream(\"serialize.out\"); ObjectOutputStream objectOutputStream = new ObjectOutputStream(fileOutputStream); objectOutputStream.writeObject(serialize); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 接下来，我们编写序列化测试代码TestWrite.class，将新建一个TestObject对象，并利用Java内置序列化，将测试对象序列化后存入serialize.out文件中。123456789101112131415import java.io.FileInputStream;import java.io.ObjectInputStream;public class TestRead &#123; public static void main(String[] args) &#123; try &#123; FileInputStream fileInputStream = new FileInputStream(\"serialize.out\"); ObjectInputStream objectInputStream = new ObjectInputStream(fileInputStream); TestObject serialize = (TestObject) objectInputStream.readObject(); System.out.println(serialize); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 最后，我们编写反序列化测试代码TestRead.class，读取serialize.out文件中的数据，并对其反序列化为Java对象，类型转换为TestObject类，并打印出该对象，输出结果为：a: 7 b: 18。由此我们完成了序列化和反序列的过程。 serialVersionUID作用和用法 现在出现另一个问题，如果在我们读取serialize.out文件，进行反序列化时，我们修改了TestObject.class类，在类中添加新的字段int c，这时候我们进行反序列化，程序会报异常java.io.InvalidClassException: TestObject; 之所以会报异常，是因为Java在序列化和反序列化的过程中，会涉及到很多检查，其中就包括对serialVersionUID的检查。serialVersionUID可以由类指定，也可以不指定，如果不指定(比如我们上面例子中)，Java会根据class计算serialVersionUID，只要类变化，计算出来的serialVersionUID也会发生变化。在反序列化时，如果发现类中serialVersionUID与之前序列化的serialVersionUID对不上号，就会抛出java.io.InvalidClassException异常。所以我们上面例子中，在添加了新的字段，改变了TestObject.class类，该类的serialVersionUID与当初序列化时的发生了改变，所以报出异常。 但是在实际应用中，我们经常会碰到这种需求，比如：在大数据处理中，Flume日志收集组件将结构化日志对应的类对象进行序列化，通过网络传输写入RocketMQ队列，实时数据分析集群JStorm中存在多个Topology作业，都从RocketMQ队列中读取序列化的数据，并反序列化后得到相应的结构化日志对象。这是典型的大数据实时处理架构，在实际应用中，如果我们修改了Flume端结构日志对应的类结构，如添加或删减了某些字段，这时候，我们不得不清空RocketMQ队列中修改前的数据，并对JStorm中使用到该类的多个Topology作业做出调整，这是一件相当繁琐的事情。显示这不符合程序开发的规则，代码不能实现“向下兼容”。 我们真正需要的是，将M类对象序列化之后，需要版本升级，修改M类，希望反序列化的时候还能识别之前版本的序列化对象。要实现这种“向下兼容”，相当简单，只需要我们显示的指定序列化对象的serialVersionUID即可。 serialVersionUID有两种显示的生成方式，一种是默认的1L，一种是根据类及其成员等属性生成的一个64位哈希码。我们使用的集成开发环境Eclipse和Intellij IDEA可以帮我们自动生成serialVersionUID。以IDEA为例，IDEA默认并没有开启自动生成serialVersionUID的功能，需要我们手动开启该功能，在File-&gt;Setting-&gt;Inspections-&gt;Serializable class without “serialVersionUID”选项上打上对勾，即可开启该功能。 开启该功能后，我们在实现了Serializable接口的类名上按下Alt+Enter，并选择添加serialVersionUID即可。 加入serialVersionUID后的TestObject.clas如下所示，然后我们重新运行TestWrite，对其该类对象进行序列化并写入serialize.out文件，然后我们TestObject.class类中进行添加int c字段或删除long b字段等操作，发现TestRead可以正常反序列化得到TestObject对象，完美实现了“向下兼容”。123456789101112131415161718import java.io.Serializable;public class TestObject implements Serializable &#123; private static final long serialVersionUID = -4761019091046434326L; private int a; private long b; public TestObject(int a, long b)&#123; this.a = a; this.b = b; &#125; @Override public String toString() &#123; return \"a: \" + a + \" b: \"+ b; &#125;&#125; 当序列化时类不变，反序列化时，在TestObject类中添加字段，则新增字段在反序列化时被赋予默认值；若删除了字段，则在反序列化时会忽略被删减的字段。反过来，序列化时类增加或删减字段，反序列化时类不变的情况，与以上情况相同。","tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"}]},{"title":"《Java编程思想》读书笔记——1","date":"2017-02-12T14:16:47.000Z","path":"2017/02/12/Java编程思想读书笔记1/","text":"《Java编程思想》第2章——一切都是对象 在处理类型的层次结构时，经常想把一个对象不当作它所属的特定类型来对待，而是将其当作基类的对象来对待。这使得人们可以编写出不依赖于特定类型的代码。 Java完全采用了动态内存分配方式。每当想要创建新对象时，就要使用new关键字来构建此对象的动态实例。 Java提供了被称为“垃圾回收器”的机制，它可以自动发现对象何时不再被使用，并继而销毁它。 异常是一种对象，它从出错地点被“抛出”，并被专门设计用来处理特定类型错误的相应的异常处理器“捕获”。 《Java编程思想》第2章——一切都是对象 Java中的对象必须由用户自己创建。 Java中的基本类型存储在堆栈中，对象存储在堆中，用堆进行存储分配和清理可能比堆栈进行存储分配需要更多时间，但是灵活性要更高。 基本类型具有的包装器类，使得可以在堆中创建一个非基本对象，用来表示对应的基本类型。 在Java中不需要销毁对象，该工作由Java的垃圾回收机制自动完成。 在Java中你所做的全部工作就是定义类，产生那些类的对象，以及发送信息给这些对象。 若类的某个成员是基本数据类型，即使没有进行初始化，Java也会确保它获得一个默认值(当基本类型变量作为类的成员使用时，Java才确保给定器默认值，但该方法并不适用于局部变量，即并非类的字段时)。 Java中的方法只能作为类的一部分来创建，不能单独存在。同时，Java中的所有代码都必须写在类里。 Java消除了所谓的“向前引用”问题。 javadoc标签：诸如@author、@version、@param、@return、@throws、@Deprecated。 Java代码采用“驼峰风格”：类名的首字母要大写，如果类名由多个单词构成，其中每个单词的首字母都采用大写形式，方法、字段(成员变量)以及对象引用名称等与类名的风格一样，只是第一个字母采用小写。 《Java编程思想》第3章——操作符 Java的基本类型存储了实际的数值，而并非指向一个对象的引用，所以在为其赋值时，是直接将一个地方的内容复制到了另一个地方；而对一个对象进行操作时，我们真正操作的是对象的引用。 整数除法会直接去掉结果的小数位，而不是四舍五入地结果。 对对象使用“==”和“!=”操作符时，比较的是对象的引用，而并非对象的实际值；如果想比较两个对象的实际内容是否相同，必须使用所有对象都适用的特殊方法equals()。但这个方法不适用于基本类型，基本类型直接使用==和!=即可。但值得注意的是，equals()的默认行为是比较引用，所以除非在自己的新类中覆盖equals()方法，否则不可能表现出我们希望的行为。 与在C及C++中不同的是：不可以将一个非布尔值当作布尔值在逻辑表达式中使用。比如不允许我们将一个数字作为布尔值使用。 注意逻辑操作符的“短路”现象，如果所有逻辑表达式都有一部分不必计算，那将获得潜在的性能提升。 移位操作符只可用来处理整数类型。 Java允许我们把任何基本类型数据转换成别的基本数据类型，但布尔型除外，后者根本不允许进行任何类型的转换处理。 通常，表达式中出现的最大的数据类型决定了表达式最终结果的数据类型。 当将一种类型转换成一种较小的类型时，必须留意“窄化转换”的结果，否则会在类型转换过程中不知不觉地丢失了信息。 《Java编程思想》第4章——控制执行流程 如果在返回void的方法中没有return语句，那么在该方法的结尾处会有一个隐式的return。 一般的continue会退回最内层循环的开头，并继续执行。 带标签的continue会到达标签的位置，并重新进入那个标签后面紧接的循环(即标签所指的循环)。 一般的break会中断并跳出当前循环。 带标签的break会中断并跳出标签所指的循环。","tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"}]},{"title":"开启博客之旅","date":"2017-01-14T01:58:19.000Z","path":"2017/01/14/开启博客之旅/","text":"博客初衷——技术小白的突围 上周阅读《程序员修炼之道——从小工到专家》一书，书中介绍的种种经验，使我受益匪浅的同时也让我更深刻的体会到自己经验的缺乏，深深的无力感。虽说在大学校园也做了不少项目，但都是小打小闹，往往一个人就把方案设计、软件编写和测试全搞定，根本不会设计到团队分工、设计规范、接口统一、软件模块的正交设计等。在阅读《程序员修炼之道》时深深感受到这些经验的宝贵，激发了我把学习过程的点点滴滴记录下来的欲望。我想如果今后我把自己在技术开发和学习中的体会和经验完整的记录下来，也许在不远的将来我也能够成为“指点江山”的大牛！（程序猿也就这么点追求啦）。 正是如此，才有了这个博客，来见证一个技术小白的突围！ 博客搭建——终于有了属于自己的个人博客 网上有很多提供博客服务的网站，csdn、博客园等，但想着程序猿就要做点与众不同的事情（自己折磨自己，哈哈），于是便有了使用github搭建个人博客的想法。通过网上查阅资料，博客搭建在经过gitpage建立、Node.js安装、hexo安装和拷贝主题之后总算是搭建完成了，看着属于自己的博客一步步建立起来，这个过程还是有那么一点小小成就感的(^▽^)。 博客搭建完成好方才体会到hexo用来搭建个人博客的确是方便许多，使用命令hexo new &quot;博客文章名&quot;新建markdown文件，然后通过Windows的MarkDownPad这个网上推荐的Windows下MarkDown编辑工具进行博客文章的编辑并可实时预览，编辑完成后hexo g生成html，hexo s即可实现本地浏览器预览，hexo d一键将本地编写好的博客推送至gitpage上，So Easy！这里有我当初搭建博客时参考的博客，对我帮助很大，供大家参考：使用hexo+github搭建免费个人博客详细教程 博客规划——大数据技术 作为一名大数据开发程序猿，工作中和未来发展方向都是大数据开发，因此博客中主要以大数据技术为主，记录我在工作和学习中的点点滴滴(也许偶尔也会文艺一下写点随笔^▽^)，目前工作中我主要使用的大数据技术方面的组件一是用于大数据实时分析的Storm，确切的说是JStorm，二是用于数据搜集的Flume，对于如今非常火热的Spark，我个人也非常感兴趣，正着手进行Scala语言的学习，所以未来很长一段时间博客主要以Storm、Flume、Spark的技术学习为主。 结语 在完成第一篇博客的最后，兴奋之余反而有点不知所措，当初跃跃欲试想要写好多东西的冲动，当真正坐到电脑前敲击键盘时，脑袋空空如也，这正说明了自己技术积累的匮乏，缺乏一定的沉淀。。。正如网上所言，博客真正吸引人的不是主题、页面多么的花哨，而是其中的内容，我的个人博客还有很长的路要走，我的突围之路也有很长的路要走，在路上，唯有坚持！","tags":[{"name":"随笔","slug":"随笔","permalink":"http://yoursite.com/tags/随笔/"}]}]